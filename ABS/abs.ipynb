{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST \n",
    "Creating a trained netword for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST neural net\n",
    "class MNIST(nn.Module):\n",
    "\n",
    "    def __init__ (self):\n",
    "        super(MNIST, self).__init__()\n",
    "        self.pic_size = 28 * 28\n",
    "        self.classes = 10\n",
    "        hidden_layer_size = 100\n",
    "\n",
    "        self.lin1 = nn.Linear(self.pic_size, hidden_layer_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.lin2 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.lin3 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.lin_final = nn.Linear(hidden_layer_size, self.classes)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.pic_size)\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.lin_final(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "network = MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    '/files/', \n",
    "    train=True,\n",
    "    download=True, \n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    '/files/', \n",
    "    download=True, \n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guilh\\AppData\\Local\\Temp\\ipykernel_15108\\1990201869.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 597.8067016601562\n",
      "epoch: 1, loss: 165.23733520507812\n",
      "epoch: 2, loss: 110.61743927001953\n",
      "epoch: 3, loss: 85.61473846435547\n",
      "epoch: 4, loss: 68.15642547607422\n",
      "epoch: 5, loss: 57.68968200683594\n",
      "epoch: 6, loss: 48.515567779541016\n",
      "epoch: 7, loss: 40.933692932128906\n",
      "epoch: 8, loss: 35.55086898803711\n",
      "epoch: 9, loss: 29.528057098388672\n"
     ]
    }
   ],
   "source": [
    "def train(\n",
    "    network,\n",
    "    train_loader,\n",
    "    criterion = nn.CrossEntropyLoss(),\n",
    "    optimizer = optim.SGD,\n",
    "    num_epochs = 10,\n",
    "    learning_rate = .1,\n",
    "):\n",
    "    optimizer = optimizer(network.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch_idx, sample in enumerate(train_loader):\n",
    "            pics, labels = sample\n",
    "            labels = F.one_hot(labels, num_classes=10)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = network(pics)\n",
    "            loss = criterion(outputs, labels.to(torch.float))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss\n",
    "\n",
    "        print(f\"epoch: {epoch}, loss: {epoch_loss}\")\n",
    "\n",
    "train(network, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guilh\\AppData\\Local\\Temp\\ipykernel_15108\\1990201869.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: avg loss 0.000602699990849942, accuracy: 98.73%\n"
     ]
    }
   ],
   "source": [
    "def test(\n",
    "    network, \n",
    "    test_loader,\n",
    "    criterion = nn.CrossEntropyLoss(),\n",
    "): \n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): \n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            correct += sum(target == output.argmax(axis=1))\n",
    "            labels = F.one_hot(target, num_classes=10)\n",
    "            test_loss += criterion(output, labels.to(torch.float))\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        print(f\"Test set: avg loss {test_loss}, accuracy: {100*correct/len(test_loader.dataset):.2f}%\")\n",
    "    \n",
    "test(network, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST(\n",
      "  (lin1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (lin2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (lin3): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (lin_final): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (softmax): LogSoftmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_loader = torch.utils.data.DataLoader(\n",
    "    testset, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 5\n",
    "benign_image = abs_loader.dataset[seed][0][0]\n",
    "label = abs_loader.dataset[seed][1]\n",
    "plt.imshow(benign_image, cmap='gray')\n",
    "plt.title(label)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guilh\\AppData\\Local\\Temp\\ipykernel_15108\\1990201869.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-14.7765,  -7.9678,  -0.0411,  -3.7311, -14.2430, -13.4170, -18.9131,\n",
       "          -4.1771,  -7.3584, -11.3856]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations = network(benign_image)\n",
    "activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([100, 784]),\n",
       " torch.Size([100]),\n",
       " torch.Size([100, 100]),\n",
       " torch.Size([100]),\n",
       " torch.Size([100, 100]),\n",
       " torch.Size([100]),\n",
       " torch.Size([10, 100]),\n",
       " torch.Size([10])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(network.parameters())\n",
    "\n",
    "[par.shape for par in params]\n",
    "# we see pairs of matrices and biases\n",
    "# to change the individual neuron activation, we need to change the biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1765, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_layer = params[-1]\n",
    "last_layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guilh\\AppData\\Local\\Temp\\ipykernel_15108\\1990201869.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-14.7765,  -7.9678,  -0.0411,  -3.7311, -14.2430, -13.4170, -18.9131,\n",
       "          -4.1771,  -7.3584, -11.3856]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(benign_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lin1.weight',\n",
       "              tensor([[ 0.0311,  0.0173, -0.0059,  ...,  0.0008, -0.0298, -0.0350],\n",
       "                      [ 0.0209, -0.0280,  0.0352,  ..., -0.0153, -0.0272,  0.0118],\n",
       "                      [ 0.0069, -0.0320, -0.0029,  ..., -0.0310, -0.0119,  0.0145],\n",
       "                      ...,\n",
       "                      [-0.0189,  0.0327, -0.0299,  ..., -0.0051,  0.0337, -0.0025],\n",
       "                      [ 0.0324, -0.0264, -0.0013,  ...,  0.0163, -0.0043, -0.0111],\n",
       "                      [ 0.0022,  0.0349,  0.0207,  ...,  0.0181, -0.0292, -0.0143]])),\n",
       "             ('lin1.bias',\n",
       "              tensor([ 0.0465, -0.0033, -0.1511,  0.0287,  0.1649, -0.1343,  0.0510,  0.1876,\n",
       "                      -0.1513,  0.0083,  0.1440, -0.0435,  0.0014, -0.0116,  0.0976, -0.0441,\n",
       "                       0.1341, -0.0864,  0.0403,  0.0917,  0.0038,  0.0452, -0.0419,  0.0283,\n",
       "                       0.0599,  0.0080,  0.2478, -0.0437,  0.0588, -0.0084,  0.1189,  0.0085,\n",
       "                      -0.0721,  0.0858,  0.1145, -0.0134,  0.0167,  0.0522,  0.0377,  0.0060,\n",
       "                       0.2262,  0.0358, -0.0522, -0.1532, -0.0143,  0.0007, -0.0262, -0.2029,\n",
       "                       0.0081, -0.0347, -0.0088,  0.0284,  0.0068,  0.0355,  0.0280,  0.0786,\n",
       "                       0.0071, -0.0227,  0.0218,  0.1082, -0.0783,  0.0709,  0.0027, -0.0164,\n",
       "                       0.0094,  0.0281,  0.0545,  0.1268,  0.2084,  0.1372, -0.0493,  0.1987,\n",
       "                      -0.1204,  0.0004, -0.0405,  0.0568, -0.1152, -0.0383,  0.0339,  0.0955,\n",
       "                      -0.0067,  0.0362, -0.0995, -0.0250,  0.0509,  0.1126,  0.0075,  0.0208,\n",
       "                       0.0637,  0.1511, -0.0470,  0.1007, -0.0665, -0.0082, -0.0742,  0.0353,\n",
       "                      -0.0091, -0.0484,  0.0125,  0.0009])),\n",
       "             ('lin2.weight',\n",
       "              tensor([[ 0.1280, -0.1163, -0.0307,  ...,  0.1738, -0.0663,  0.0138],\n",
       "                      [ 0.1026,  0.0709, -0.1265,  ..., -0.1075, -0.0084,  0.0012],\n",
       "                      [ 0.0867, -0.0343,  0.1869,  ..., -0.0609, -0.1910,  0.1006],\n",
       "                      ...,\n",
       "                      [ 0.1114, -0.0802,  0.0449,  ...,  0.0306,  0.0482, -0.0821],\n",
       "                      [-0.0139,  0.0747,  0.0177,  ..., -0.0331, -0.0045, -0.0466],\n",
       "                      [-0.1813,  0.0329,  0.1118,  ..., -0.0604, -0.0071,  0.3296]])),\n",
       "             ('lin2.bias',\n",
       "              tensor([-1.1419e-02,  1.5769e-01,  2.6168e-02,  2.6170e-02,  4.3279e-02,\n",
       "                      -8.7592e-02, -1.4953e-02,  1.1382e-01, -7.3402e-02,  4.9264e-02,\n",
       "                       1.3170e-01, -1.0823e-02, -1.2197e-01, -6.3455e-02,  1.1652e-01,\n",
       "                      -5.5181e-02,  8.0356e-02, -2.9066e-02, -8.4262e-02, -8.2645e-02,\n",
       "                       2.1017e-03,  1.0475e-01,  1.1044e-02,  1.4175e-01,  2.1651e-01,\n",
       "                      -3.4223e-02,  8.6107e-03, -3.2275e-02, -9.0141e-02, -7.2111e-02,\n",
       "                      -8.3475e-02,  1.4569e-01,  7.1077e-02, -6.6061e-02, -3.8577e-02,\n",
       "                       1.1425e-01,  1.4849e-01,  2.9435e-03, -2.4330e-02, -6.6657e-03,\n",
       "                       6.8731e-02,  2.1742e-01, -2.7644e-02, -1.0956e-01,  7.3046e-02,\n",
       "                       8.5675e-02, -7.4243e-02,  1.7846e-01,  7.6696e-02, -2.6534e-02,\n",
       "                       2.2700e-02,  2.3011e-03,  2.8351e-02, -6.1368e-02, -7.8648e-02,\n",
       "                       1.8016e-01, -4.7651e-02, -3.3602e-03, -1.2460e-02,  1.1623e-01,\n",
       "                      -9.6113e-02, -2.2552e-02,  1.7788e-01,  9.3402e-02,  3.6460e-02,\n",
       "                       1.6989e-01,  1.4205e-01,  8.4157e-02, -1.7595e-02,  2.9095e-02,\n",
       "                      -4.8026e-02, -4.7096e-02,  3.3632e-02,  5.9113e-03,  2.2951e-02,\n",
       "                       1.0081e-01, -5.8617e-02, -1.4854e-03, -1.1382e-02,  1.6258e-01,\n",
       "                      -1.8615e-02,  1.5317e-01,  9.4950e-02, -9.5520e-03, -7.8454e-02,\n",
       "                       3.9971e-02,  8.4907e-06,  1.1475e-01,  7.1135e-02,  1.0419e-01,\n",
       "                       8.0485e-03,  5.4979e-03,  1.5592e-01, -1.1692e-01,  1.5709e-01,\n",
       "                       8.2398e-02,  1.8433e-01, -9.2341e-02, -2.1474e-02,  7.0626e-02])),\n",
       "             ('lin3.weight',\n",
       "              tensor([[ 0.0993, -0.1135,  0.0895,  ..., -0.0695,  0.1005,  0.2884],\n",
       "                      [-0.0189,  0.1396,  0.0872,  ...,  0.1588, -0.1224, -0.2598],\n",
       "                      [ 0.0592, -0.0834,  0.1532,  ..., -0.0437, -0.0952,  0.1329],\n",
       "                      ...,\n",
       "                      [-0.0441, -0.0019,  0.1335,  ...,  0.0384,  0.1001, -0.0356],\n",
       "                      [ 0.1080, -0.1061,  0.0299,  ...,  0.0223, -0.0700, -0.1299],\n",
       "                      [-0.0862, -0.0957, -0.0544,  ..., -0.0584, -0.0808,  0.0092]])),\n",
       "             ('lin3.bias',\n",
       "              tensor([ 1.0502e-01,  1.3432e-01,  1.6085e-01,  1.7930e-01, -5.0992e-02,\n",
       "                       2.2414e-01, -4.3850e-02,  5.9709e-02, -1.7389e-02, -3.4919e-02,\n",
       "                      -3.7137e-02,  1.0663e-02,  6.8415e-02,  8.6476e-03, -1.3802e-01,\n",
       "                       2.4296e-02, -1.5979e-03,  4.0018e-02,  4.1360e-02, -5.9014e-02,\n",
       "                       7.1056e-02,  1.0758e-01,  1.0331e-01,  1.4009e-02,  3.4687e-02,\n",
       "                       1.5138e-01,  7.9617e-02,  1.2906e-01, -4.9620e-02,  6.3767e-02,\n",
       "                       4.2032e-02,  7.8357e-02,  8.4089e-02, -4.0446e-02,  2.0226e-01,\n",
       "                       1.7980e-01, -6.9768e-02,  5.1400e-02,  1.0452e-03,  7.5460e-02,\n",
       "                      -5.1076e-02,  9.2086e-02, -2.7454e-06,  1.0921e-01,  1.0827e-01,\n",
       "                       1.4036e-01, -7.1406e-02,  4.2790e-02,  1.2770e-01, -3.6540e-02,\n",
       "                       4.0916e-02,  9.3536e-02,  5.3293e-02,  4.8926e-02,  2.1342e-02,\n",
       "                       1.9481e-01,  6.2729e-02,  4.5266e-02,  1.3673e-01, -5.5895e-02,\n",
       "                       8.5690e-02,  9.6959e-02,  2.7513e-01, -2.3583e-02,  5.7491e-03,\n",
       "                      -5.1461e-02,  1.5703e-01, -2.9466e-02, -4.6732e-02,  2.6364e-02,\n",
       "                       1.6098e-01, -8.8530e-02,  6.9844e-03, -9.3572e-02,  1.3970e-01,\n",
       "                       1.5355e-01, -4.5796e-03, -9.3082e-02,  1.8347e-01,  7.9467e-02,\n",
       "                       5.1106e-02,  2.5375e-02,  9.9617e-02, -3.6846e-02, -1.0459e-01,\n",
       "                       5.9571e-02,  2.4059e-01, -1.4229e-02,  1.1166e-01,  1.2056e-01,\n",
       "                       1.9772e-01,  1.0700e-01,  9.9208e-02,  1.8236e-02, -1.0482e-02,\n",
       "                       2.5683e-02,  3.3305e-02,  1.0323e-01,  3.8450e-02,  1.5576e-01])),\n",
       "             ('lin_final.weight',\n",
       "              tensor([[ 1.6426e-01, -2.3266e-01, -2.4799e-01, -2.5303e-01, -1.6386e-01,\n",
       "                       -2.6615e-01, -1.3414e-02, -3.3194e-01, -6.5519e-02, -2.8574e-01,\n",
       "                        3.8175e-02,  2.2374e-02,  2.7231e-02, -4.2271e-02, -8.1785e-02,\n",
       "                       -3.3933e-02,  8.4273e-02,  3.1769e-01, -5.9052e-02, -3.9742e-02,\n",
       "                       -8.3174e-02, -1.1479e-01,  1.7187e-01,  4.1714e-02, -1.2291e-01,\n",
       "                       -2.7023e-01,  1.2038e-01, -3.6320e-01,  5.6973e-02,  3.0761e-01,\n",
       "                       -1.2795e-01,  2.5423e-01, -1.0347e-01,  6.0157e-02, -1.4690e-01,\n",
       "                       -2.5952e-01,  3.5901e-01,  3.5426e-01, -2.1017e-01, -3.9659e-02,\n",
       "                        6.0857e-01, -1.5341e-01,  1.9922e-01, -7.1737e-02,  1.4817e-01,\n",
       "                        6.8642e-02,  4.8389e-01, -6.0598e-02,  2.3782e-01,  5.4080e-02,\n",
       "                       -5.6420e-03,  1.6631e-01, -2.8324e-01,  2.7441e-01, -7.1169e-03,\n",
       "                       -3.6346e-01,  1.8419e-01,  3.5006e-02, -1.3364e-01, -2.4485e-01,\n",
       "                        2.3066e-02, -8.9309e-02, -2.7199e-01, -4.2777e-02, -1.2836e-01,\n",
       "                        6.0628e-03, -2.3384e-01,  3.5691e-02, -7.4891e-02,  2.8054e-01,\n",
       "                        1.7958e-01,  3.6352e-02,  1.9557e-01, -5.2975e-02, -2.0492e-02,\n",
       "                       -3.6741e-01,  5.3804e-02,  8.1953e-02, -2.7143e-01,  5.0147e-02,\n",
       "                        2.6409e-01, -2.5138e-01,  1.4791e-01,  5.2669e-02, -9.4029e-02,\n",
       "                       -8.6506e-02, -1.9537e-01, -2.2921e-02,  1.1554e-01, -2.7157e-01,\n",
       "                       -3.8985e-01, -2.6622e-01,  3.8514e-01, -1.8056e-01,  5.3603e-01,\n",
       "                       -2.4211e-01, -3.4328e-02, -1.0875e-01, -1.1371e-01,  2.4886e-02],\n",
       "                      [-3.3944e-02, -1.3822e-01,  1.0086e-01,  2.6001e-01, -1.0189e-01,\n",
       "                        1.2972e-01, -3.0665e-02,  2.3392e-01,  3.3085e-02,  1.1484e-01,\n",
       "                        8.0185e-02, -1.0618e-01,  8.2613e-02, -8.5802e-03,  1.3509e-01,\n",
       "                        9.4628e-02,  1.8122e-02, -3.9603e-02, -7.5885e-02,  2.3510e-01,\n",
       "                        8.4023e-02, -1.5770e-01, -3.2409e-02, -1.7795e-01, -3.0746e-02,\n",
       "                        1.2644e-01, -2.3952e-01,  3.5324e-01, -1.3026e-02, -1.6869e-01,\n",
       "                        1.7056e-01, -1.7355e-01,  2.7042e-01,  1.6497e-01, -3.8629e-01,\n",
       "                       -1.8692e-01,  1.9735e-02, -4.5117e-01,  3.1711e-02, -7.6034e-02,\n",
       "                        1.5858e-02,  1.9829e-01, -2.1671e-01,  4.3825e-01,  1.8150e-01,\n",
       "                        9.8193e-03, -1.3282e-01, -1.2099e-01,  1.8896e-02,  8.0689e-02,\n",
       "                       -2.6401e-01, -2.7687e-01, -3.6537e-01,  2.4692e-01,  4.1998e-01,\n",
       "                        4.5014e-01, -1.1904e-01, -4.2336e-01,  3.5669e-01, -1.3963e-01,\n",
       "                        3.6970e-02,  3.2715e-01,  3.8336e-01,  5.7963e-03, -2.8188e-02,\n",
       "                        6.4558e-02,  3.2032e-01, -8.7634e-02,  4.6474e-02, -2.2101e-02,\n",
       "                       -1.5157e-01,  1.0342e-01, -2.5189e-01, -2.7208e-02,  5.1264e-01,\n",
       "                        2.5164e-01, -4.0159e-01, -5.6094e-02,  3.4933e-01,  2.1435e-02,\n",
       "                       -9.3779e-02, -6.9194e-02, -7.3686e-02, -4.0586e-02,  2.7096e-01,\n",
       "                       -1.4101e-01,  2.4940e-01, -1.4382e-01, -3.4014e-01, -5.0624e-02,\n",
       "                        5.0986e-02, -2.0027e-01, -3.8183e-01,  3.8097e-01, -2.7731e-01,\n",
       "                        1.4747e-01, -6.6159e-02, -4.4954e-01, -2.7095e-01, -1.4411e-01],\n",
       "                      [ 5.0268e-01, -3.9831e-01,  1.7559e-01, -2.6090e-01,  2.9316e-01,\n",
       "                        2.0038e-01, -5.3564e-02,  2.1098e-01, -1.1863e-01,  1.2836e-01,\n",
       "                       -8.2304e-02,  3.2984e-02,  8.8006e-03, -3.9238e-02, -8.0379e-02,\n",
       "                       -8.6633e-03,  9.1171e-02, -3.1964e-01,  5.3926e-02, -5.9176e-02,\n",
       "                        2.6657e-01, -7.7955e-02,  5.7727e-01, -1.3628e-01, -2.5161e-02,\n",
       "                        1.2288e-02, -1.0777e-01,  2.0427e-01,  1.1420e-01,  3.7136e-01,\n",
       "                        1.4761e-01, -1.8016e-01,  5.1408e-01, -2.8529e-02,  3.9270e-03,\n",
       "                       -3.7132e-01, -5.2389e-02,  2.1534e-01, -1.3461e-01, -3.5000e-02,\n",
       "                       -8.0940e-02, -3.7572e-02,  2.4015e-01, -2.2506e-01,  1.7381e-01,\n",
       "                        2.5078e-02,  1.1036e-01, -3.1439e-01,  2.7062e-02, -5.4317e-02,\n",
       "                        8.5700e-03, -1.7518e-01, -6.7231e-02, -4.6874e-02,  3.7330e-02,\n",
       "                       -7.6963e-02, -2.6412e-01, -1.1413e-01, -8.9919e-02,  1.9775e-01,\n",
       "                        1.8439e-01, -1.3281e-01, -1.3672e-01,  9.4340e-02,  2.0350e-01,\n",
       "                       -4.8509e-02,  1.3477e-01, -2.5803e-02,  4.6497e-02, -9.9452e-02,\n",
       "                        2.6204e-01, -2.7787e-01,  9.1180e-02,  1.3888e-02, -3.3730e-01,\n",
       "                       -1.3731e-01,  2.7265e-01, -6.0197e-02, -3.8535e-01,  4.1554e-01,\n",
       "                        9.5018e-03, -2.2802e-02, -4.3128e-03,  6.5214e-02, -1.8214e-02,\n",
       "                        1.0087e-01,  1.2528e-01, -2.6750e-01,  2.3048e-01, -1.0525e-01,\n",
       "                        2.4455e-01,  1.4766e-01,  1.4212e-01,  1.9363e-01, -3.7469e-02,\n",
       "                       -2.2751e-01, -1.7467e-01, -1.9255e-01,  7.2229e-02,  1.5576e-01],\n",
       "                      [-9.7993e-02, -3.7934e-01,  1.0124e-01,  1.6777e-01,  2.1097e-01,\n",
       "                        2.5125e-01,  6.9389e-02,  5.6484e-01, -1.0112e-01,  3.7918e-01,\n",
       "                       -6.1965e-02, -4.6285e-02,  5.7400e-02, -5.0685e-02,  1.6414e-01,\n",
       "                       -3.3715e-02,  3.3766e-02,  1.1966e-01, -2.9895e-02,  2.6544e-01,\n",
       "                        1.9639e-01, -2.6129e-02, -1.3216e-01,  3.6488e-01,  5.2348e-02,\n",
       "                        4.4328e-01, -2.9602e-01, -4.4507e-01, -7.3220e-02, -1.3115e-01,\n",
       "                       -2.1654e-01,  6.8152e-02, -1.4306e-01, -3.6660e-02,  3.1079e-01,\n",
       "                       -3.3511e-01, -1.8725e-01, -1.3328e-01,  4.8045e-02,  2.1325e-01,\n",
       "                       -2.6174e-01, -4.7851e-02, -4.3920e-02,  1.3564e-01,  3.5505e-01,\n",
       "                       -2.7925e-01,  2.5962e-02, -1.1112e-01, -2.4363e-01,  5.7960e-04,\n",
       "                       -2.4755e-01,  2.5290e-01, -9.5952e-02, -2.9356e-01, -2.8006e-01,\n",
       "                       -1.7852e-01, -1.9950e-01,  2.5680e-01, -4.3193e-01,  1.2146e-01,\n",
       "                       -1.2314e-01, -1.9589e-01, -8.1134e-02,  2.4947e-02,  1.9715e-01,\n",
       "                       -3.3176e-02, -7.4208e-02,  8.5778e-02, -1.0873e-01, -1.4891e-01,\n",
       "                       -2.1134e-01, -8.0369e-04,  2.1523e-01, -5.2290e-03, -1.4508e-01,\n",
       "                        2.8215e-01,  3.7987e-01,  7.7477e-02,  9.8441e-02, -2.1322e-02,\n",
       "                        1.5087e-01, -3.2302e-01, -1.1733e-01,  7.1018e-02,  6.1397e-02,\n",
       "                       -5.5350e-02, -2.9480e-01,  3.0077e-01,  2.8529e-01, -6.0521e-02,\n",
       "                       -2.2857e-02,  3.1730e-01,  5.6002e-02,  1.5952e-01, -4.0317e-01,\n",
       "                        5.1084e-01, -4.4331e-02,  3.3415e-01, -1.4111e-01, -1.7141e-01],\n",
       "                      [-1.9613e-01,  4.9542e-01, -2.7288e-01, -1.2027e-02,  1.6465e-01,\n",
       "                        8.2351e-02,  2.4733e-03,  1.5148e-01, -2.2708e-01, -4.2383e-02,\n",
       "                       -7.6070e-02,  2.0728e-01,  8.3314e-02, -9.6347e-02,  2.6717e-02,\n",
       "                        3.8212e-02, -8.3834e-03, -2.1099e-02, -8.8651e-02,  7.6875e-02,\n",
       "                       -2.5306e-01,  2.3269e-01, -2.5225e-02, -2.1999e-01, -1.4584e-01,\n",
       "                       -2.3421e-01,  3.0838e-01,  3.8050e-01, -1.4523e-02,  1.2626e-01,\n",
       "                        1.9630e-01, -3.6946e-01, -1.5273e-01,  1.9689e-02, -2.7875e-02,\n",
       "                        2.3709e-01, -4.8453e-02,  1.4003e-01,  5.1563e-01, -1.6647e-01,\n",
       "                        5.1448e-02, -6.2540e-02, -9.4202e-02,  2.4620e-02, -3.6355e-01,\n",
       "                       -1.8442e-01, -1.1470e-01, -6.4876e-02, -1.4718e-01,  1.5415e-01,\n",
       "                        2.1852e-02, -2.0166e-01,  2.5943e-01, -3.2940e-01, -1.3722e-02,\n",
       "                        1.9390e-01,  6.3798e-02,  1.2116e-01,  2.0451e-01,  2.7888e-01,\n",
       "                        7.4555e-01, -1.6968e-01, -3.4358e-01,  9.4719e-02, -1.2131e-01,\n",
       "                       -4.7343e-02, -2.6663e-01, -4.0530e-02, -7.7324e-02,  2.1628e-01,\n",
       "                        1.5737e-01, -1.6165e-01, -2.9234e-01,  5.3886e-02,  1.8314e-01,\n",
       "                        9.4962e-02, -1.5845e-01, -5.5251e-02, -1.0165e-01, -1.2261e-01,\n",
       "                       -9.3116e-02,  8.2392e-02,  5.0349e-01,  5.8689e-02,  7.0586e-02,\n",
       "                        4.5360e-01,  2.8381e-01,  1.0009e-01, -5.2392e-01,  2.5330e-01,\n",
       "                       -3.4267e-01,  5.8644e-01, -3.0075e-01, -4.2565e-02, -1.4209e-01,\n",
       "                       -2.2616e-01, -8.7607e-02,  9.3195e-02,  7.1983e-02,  8.7327e-02],\n",
       "                      [-4.0943e-02,  4.0047e-03,  4.6396e-03, -2.1335e-01, -6.1121e-02,\n",
       "                       -3.1888e-01, -3.4762e-02, -1.2216e-01,  2.1528e-01,  1.3900e-01,\n",
       "                       -9.4337e-02,  1.5436e-01, -7.5932e-03,  3.0677e-02,  2.5012e-01,\n",
       "                        7.2342e-02,  6.9254e-03,  4.5387e-02, -4.1781e-02, -3.3314e-01,\n",
       "                        7.5525e-02,  3.5337e-02, -2.4553e-01, -1.4810e-01,  1.3395e-01,\n",
       "                        1.5510e-01,  4.4620e-03, -1.9651e-01, -4.3863e-02, -2.4525e-01,\n",
       "                        7.6319e-02,  5.1608e-01, -1.6826e-01, -2.8702e-02,  3.5801e-01,\n",
       "                       -1.4213e-02,  1.4304e-01, -4.4054e-01, -2.6317e-01,  4.2266e-01,\n",
       "                       -2.8470e-01,  8.6485e-03, -1.4723e-01,  5.8772e-02, -2.3074e-01,\n",
       "                        6.4033e-02, -3.6928e-01,  2.4870e-01,  1.2693e-01,  5.0465e-02,\n",
       "                        2.1974e-01,  6.3782e-01,  1.3510e-01,  6.6634e-02,  2.7840e-02,\n",
       "                        2.5463e-01,  1.2914e-01,  5.8115e-01, -4.6079e-02,  4.8984e-02,\n",
       "                       -3.1714e-02,  1.2497e-02,  1.1345e-01,  5.3209e-02,  1.8941e-01,\n",
       "                       -5.6109e-02, -5.0421e-01, -8.7480e-02, -8.1666e-02, -3.7116e-01,\n",
       "                        3.7253e-01,  1.2741e-01, -1.8461e-02, -4.7511e-02, -2.3270e-01,\n",
       "                       -1.4436e-01,  1.6275e-02, -1.9538e-01,  3.9347e-01,  8.5451e-02,\n",
       "                        4.2533e-02,  2.5198e-01, -9.3179e-02, -1.8738e-02, -3.8741e-02,\n",
       "                       -5.7273e-02, -1.4704e-01, -2.7875e-01,  2.1988e-01,  1.1260e-01,\n",
       "                        3.4357e-01, -1.8473e-01,  3.9909e-01, -8.3856e-02, -3.8175e-01,\n",
       "                       -8.0744e-02,  1.1796e-01,  1.6467e-01,  8.6219e-02, -2.8453e-02],\n",
       "                      [-2.7048e-01,  2.9181e-01, -1.0087e-01, -1.5475e-01, -1.0779e-01,\n",
       "                       -4.9073e-01,  8.7046e-02, -2.5609e-01,  2.0000e-02, -1.4982e-01,\n",
       "                        9.0499e-02, -1.4289e-01,  1.5806e-02, -3.8777e-02, -5.4501e-02,\n",
       "                        5.2324e-02, -4.5303e-02,  2.1058e-01,  9.8686e-02, -1.6838e-01,\n",
       "                        1.3705e-01,  8.4730e-02, -1.7276e-02, -2.1187e-01,  1.5894e-01,\n",
       "                       -3.3537e-01, -3.7233e-01, -9.9711e-05, -3.7557e-03,  1.5139e-01,\n",
       "                       -2.2992e-01, -1.1618e-01,  3.7813e-02,  5.9519e-02, -3.2446e-01,\n",
       "                        2.8168e-01,  1.7715e-02,  4.3750e-01, -2.0842e-01, -1.2490e-01,\n",
       "                       -1.2881e-01, -1.7622e-01, -1.8294e-01, -2.0623e-02,  2.5617e-02,\n",
       "                        3.2986e-01,  5.1373e-01,  3.2109e-01,  1.3625e-01,  5.3974e-02,\n",
       "                       -7.3588e-02,  1.7400e-01, -3.6251e-01,  2.4326e-01,  2.9377e-02,\n",
       "                        2.5661e-01,  7.0830e-03, -4.1398e-01,  1.0633e-02,  4.2373e-02,\n",
       "                       -2.5816e-01,  5.1107e-01, -3.2619e-01,  3.6050e-02, -5.4502e-02,\n",
       "                        9.2310e-02, -4.2017e-01, -7.0185e-03, -5.9357e-02,  5.8253e-02,\n",
       "                        3.5882e-01,  3.0664e-01, -1.4266e-01, -4.3522e-02, -1.1830e-01,\n",
       "                       -4.3569e-01, -2.5081e-01,  2.1953e-01,  3.0490e-01, -1.9233e-01,\n",
       "                        9.1556e-02,  1.1232e-01, -2.8475e-01, -3.1088e-02, -8.6702e-02,\n",
       "                       -9.7207e-02,  2.8248e-01, -5.4755e-02, -3.4623e-01,  9.7349e-02,\n",
       "                        2.5594e-01,  1.1779e-01, -1.8320e-02,  1.5440e-01,  1.4505e-01,\n",
       "                       -3.0051e-01,  1.0611e-01, -4.9345e-01,  2.2390e-01,  2.4009e-01],\n",
       "                      [ 4.5018e-01, -5.5860e-01,  5.0715e-01,  4.3370e-01,  2.7687e-01,\n",
       "                       -1.8903e-01,  3.1364e-02,  1.0757e-01, -1.5302e-01,  4.8681e-02,\n",
       "                       -8.0468e-02,  1.5037e-01, -1.2200e-01,  1.3449e-02, -1.6858e-01,\n",
       "                       -3.3989e-02,  9.0971e-02,  3.9603e-02,  3.7192e-02,  1.9825e-01,\n",
       "                        2.0545e-01,  3.2199e-01, -1.0352e-01, -9.7219e-02, -1.0269e-01,\n",
       "                       -1.5661e-01,  3.6213e-01,  2.7817e-01,  8.1983e-02, -1.0077e-01,\n",
       "                        1.1043e-01, -4.1827e-01,  2.5723e-01, -2.7462e-02,  1.3814e-01,\n",
       "                       -1.7064e-01,  9.4925e-02, -3.1932e-01, -2.2749e-01, -1.9240e-02,\n",
       "                        2.3213e-01,  4.1687e-01,  2.1940e-01, -1.9835e-01, -3.0341e-01,\n",
       "                        5.9686e-02, -2.2063e-01, -1.5971e-01, -2.2250e-01,  1.0017e-01,\n",
       "                        2.8954e-01, -2.2844e-01,  2.0281e-01,  6.7127e-02,  3.0055e-01,\n",
       "                       -2.7777e-01,  1.8880e-02,  2.7764e-02,  3.0853e-01, -8.4878e-02,\n",
       "                       -1.7793e-01, -1.0479e-01, -4.0964e-02,  9.1541e-02, -1.2839e-01,\n",
       "                       -2.0298e-03,  4.3217e-01, -2.1963e-02,  1.0834e-03,  3.9314e-02,\n",
       "                       -6.8857e-02, -1.2145e-02, -3.0978e-01, -5.7852e-02,  3.0505e-01,\n",
       "                        6.3257e-02, -7.8669e-02, -1.4388e-02, -2.2610e-01,  2.0404e-01,\n",
       "                       -3.4805e-01, -2.1988e-01,  3.4047e-01, -2.0739e-02, -1.2377e-01,\n",
       "                       -3.0174e-01, -1.9611e-01,  5.2719e-02, -5.8513e-02, -2.4226e-01,\n",
       "                       -9.9578e-02, -4.5027e-02,  2.3688e-01,  4.9814e-02,  4.0867e-01,\n",
       "                        4.1462e-01,  1.0589e-01,  4.8052e-01, -5.9384e-02, -2.4725e-01],\n",
       "                      [-7.3558e-02,  3.1979e-01, -1.5176e-01, -2.0089e-01, -1.7578e-01,\n",
       "                        3.8591e-01,  9.2922e-02, -1.9494e-01,  9.2771e-02, -2.6718e-02,\n",
       "                        4.3566e-02,  4.0712e-02,  5.6257e-02,  5.5285e-02, -3.2633e-01,\n",
       "                        3.8445e-02,  9.2748e-02, -2.8067e-01,  2.8678e-02,  1.5903e-01,\n",
       "                       -1.6514e-01, -2.4161e-01, -1.8597e-01,  3.8622e-01, -8.2228e-02,\n",
       "                        1.1996e-01, -3.5417e-02, -6.1782e-02, -1.9857e-01, -4.5250e-01,\n",
       "                       -6.6157e-02,  2.7111e-01, -4.0732e-01, -4.3503e-02,  3.4983e-02,\n",
       "                        3.2475e-01, -3.6450e-02,  3.2031e-01, -4.2304e-01, -2.5921e-01,\n",
       "                       -2.0754e-01, -1.8126e-01,  1.8622e-02, -1.3327e-01,  3.5412e-01,\n",
       "                        2.6981e-01, -1.9425e-01,  1.6155e-01, -2.7386e-01, -1.0689e-01,\n",
       "                       -2.0732e-01, -3.4433e-01, -1.3327e-02,  2.2623e-02, -3.0324e-01,\n",
       "                       -1.6694e-01, -2.0858e-01, -1.3048e-01,  2.0112e-01, -1.2119e-01,\n",
       "                       -2.4931e-01, -1.3331e-01,  3.9850e-01,  9.3097e-02, -3.9002e-01,\n",
       "                       -7.5228e-02,  4.4749e-01, -8.7788e-02, -9.7876e-02,  1.6166e-01,\n",
       "                       -4.1079e-01, -9.5161e-03,  2.5625e-01, -1.1318e-01, -3.7807e-01,\n",
       "                        3.5575e-01,  3.3362e-01,  1.2027e-01, -3.0472e-02, -5.7128e-02,\n",
       "                        9.1574e-02,  3.5890e-01, -4.0604e-02,  6.0021e-02, -1.3477e-01,\n",
       "                       -2.3542e-01,  3.8633e-02, -1.4819e-01,  4.5192e-01,  3.4339e-01,\n",
       "                        3.0619e-01, -4.5976e-02, -4.9927e-02,  1.0889e-01, -3.8227e-01,\n",
       "                       -7.6097e-02,  2.5026e-02, -3.9602e-02, -9.0484e-02,  1.0089e-01],\n",
       "                      [-3.3354e-01,  4.0908e-01,  1.4732e-01,  7.3977e-02, -6.2747e-02,\n",
       "                        1.8813e-01,  5.6506e-02, -1.4070e-01,  1.7675e-01, -2.3827e-01,\n",
       "                        1.0103e-01, -2.3018e-01,  2.0334e-02,  3.8244e-03,  1.7410e-01,\n",
       "                       -5.6394e-02, -4.0803e-02,  3.5366e-02,  8.3271e-02, -2.9333e-01,\n",
       "                       -2.9430e-01, -6.3767e-02, -1.7989e-02,  7.7258e-03,  1.4978e-01,\n",
       "                        2.5285e-01, -3.8095e-04, -1.6991e-01,  3.2717e-02,  4.7219e-02,\n",
       "                        3.4119e-02, -1.9679e-02, -5.3439e-02,  1.1875e-01,  3.1447e-01,\n",
       "                        2.5156e-01, -1.5542e-01,  5.6293e-02,  6.4260e-01,  8.6373e-02,\n",
       "                       -5.1732e-02, -6.6410e-04, -2.7767e-02,  7.1145e-02, -3.3863e-01,\n",
       "                       -3.2731e-01, -1.4841e-01, -1.6163e-01,  4.1254e-01,  2.4400e-02,\n",
       "                        2.1264e-01,  3.7435e-02,  5.8067e-01,  1.1410e-01, -1.5334e-01,\n",
       "                       -1.0936e-01,  1.8704e-01,  1.6232e-01, -3.2592e-01, -1.0952e-01,\n",
       "                       -2.5647e-01, -1.0866e-01,  2.2759e-01,  8.7454e-02, -1.1100e-01,\n",
       "                       -8.0926e-02, -3.6001e-02,  4.3787e-02, -5.4800e-02,  5.5117e-02,\n",
       "                       -3.7654e-01, -3.2475e-02,  3.6666e-01, -1.7933e-02,  2.1506e-01,\n",
       "                        2.5657e-01, -2.2744e-01,  2.2780e-02, -1.4575e-01, -2.1754e-01,\n",
       "                        3.1230e-01,  9.7616e-02, -2.2276e-01, -1.0055e-01,  7.8254e-02,\n",
       "                        5.2289e-01, -2.7609e-01,  4.2478e-01, -5.3357e-02, -1.7700e-01,\n",
       "                       -3.7478e-01, -5.2608e-01, -2.0897e-01, -3.0325e-01,  4.4580e-01,\n",
       "                        4.0517e-01, -1.0741e-01,  3.6058e-01,  2.4796e-01, -6.6365e-03]])),\n",
       "             ('lin_final.bias',\n",
       "              tensor([-0.1765,  0.1963, -0.0228,  0.0705,  0.0947,  0.3113, -0.0556,  0.0040,\n",
       "                      -0.1660, -0.0241]))])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guilh\\AppData\\Local\\Temp\\ipykernel_15108\\1990201869.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        # print()\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.lin1.register_forward_hook(get_activation('lin12'))\n",
    "output = model(benign_image)\n",
    "# print(activation['lin1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy model\n",
    "\n",
    "class Dummy(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(Dummy, self).__init__()\n",
    "        self.lin1 = nn.Linear(2, 2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(2, 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "dummy = Dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lin1.weight',\n",
       "              tensor([[-0.1955,  0.0741],\n",
       "                      [-0.1511,  0.3093]])),\n",
       "             ('lin1.bias', tensor([-0.1308, -0.6364])),\n",
       "             ('lin2.weight',\n",
       "              tensor([[-0.0148,  0.0326],\n",
       "                      [ 0.2177, -0.5905]])),\n",
       "             ('lin2.bias', tensor([0.5789, 0.0969]))])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5789, 0.0969], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 1]\n",
    "x = torch.Tensor(x)\n",
    "dummy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5789, 2.0000])\n"
     ]
    }
   ],
   "source": [
    "# sucessful hook approach to modify activations\n",
    "def get_activation(neuron, activation):\n",
    "    def hook(model, input, output):\n",
    "        modified_out = output.detach()\n",
    "        modified_out[neuron] = activation\n",
    "        return modified_out\n",
    "    return hook\n",
    "\n",
    "hook_handle = dummy.lin2.register_forward_hook(get_activation(1, 2))\n",
    "output = dummy(x)\n",
    "print(output)\n",
    "hook_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy model with abs support\n",
    "\n",
    "from typing import Tuple, List, Callable\n",
    "\n",
    "def modify_activation(neuron, activation):\n",
    "    def hook(model, input, output):\n",
    "        modified_out = output.detach()\n",
    "        modified_out[neuron] = activation\n",
    "        return modified_out\n",
    "    return hook\n",
    "    \n",
    "def NSF(model, label: int, layer: nn.Module, neuron: int, image: torch.Tensor) -> Callable:\n",
    "    def func(x: float) -> float:\n",
    "        hook_handle = layer.register_forward_hook(modify_activation(neuron, x))\n",
    "        output = model(image)\n",
    "        hook_handle.remove()\n",
    "        return output[label]\n",
    "    return func\n",
    "\n",
    "\n",
    "def get_neurons(model) -> List[Tuple[nn.Module, int]]:\n",
    "    \"\"\"\n",
    "        Returns a list of tuples containing the layers in which the neurons are in\n",
    "        and the number of neurons in that layer.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        *[(model.lin1, i) for i in range(2)],\n",
    "        *[(model.lin2, i) for i in range(2)],\n",
    "    ]\n",
    "    \n",
    "def get_labels(model) -> List[int]:\n",
    "    \"\"\"\n",
    "        Returns the list of labels in the model, \n",
    "        aka the last layer neurons.\n",
    "    \"\"\"\n",
    "    return [0, 1] # two values for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_candidate(C, neurons, labels, base_imgs):\n",
    "    \"\"\"\n",
    "        C: the model in question\n",
    "        base_imgs: list of tuples containing (image, label)\n",
    "    \"\"\"\n",
    "    max_n = 0\n",
    "    max_l = None\n",
    "    max_v = 0\n",
    "    for layer, neuron in neurons:\n",
    "        labelLift = []\n",
    "        for label in labels:\n",
    "            min_img_v = float('inf')\n",
    "            for img in base_imgs:\n",
    "                image, img_label = img\n",
    "                if img_label == label: continue\n",
    "                x = torch.linspace(-1, 1, 100)\n",
    "                img_v = max(\n",
    "                    NSF(C, label, layer, neuron, image)(xx)\n",
    "                    for xx in x\n",
    "                    ) - C(image)[label]\n",
    "                min_img_v = min(min_img_v, img_v)\n",
    "            labelLift.append(min_img_v)\n",
    "        labelLift.sort(reverse=True)\n",
    "        n_v = labelLift[0] - labelLift[1]\n",
    "        if n_v > max_v:\n",
    "            max_v = n_v\n",
    "            max_n = neuron\n",
    "            max_l = layer\n",
    "    return max_l, max_n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=2, out_features=2, bias=True), 0)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = Dummy()\n",
    "neurons = get_neurons(C)\n",
    "labels = get_labels(C)\n",
    "base_imgs = [\n",
    "    (torch.rand(2), np.random.choice(labels)) for _ in range(20)\n",
    "]\n",
    "\n",
    "identify_candidate(C, neurons, labels, base_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# abs on MNIST\n",
    "debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_candidate(C, neurons, labels, base_imgs):\n",
    "    \"\"\"\n",
    "        C: the model in question\n",
    "        base_imgs: list of tuples containing (image, label)\n",
    "    \"\"\"\n",
    "    max_n = 0\n",
    "    max_l = None\n",
    "    max_v = float('-inf')\n",
    "    for layer, neuron in neurons:\n",
    "        labelLift = []\n",
    "        for label in labels:\n",
    "            min_img_v = float('inf')\n",
    "            for img in base_imgs:\n",
    "                image, img_label = img\n",
    "                if img_label == label: continue\n",
    "                x = torch.linspace(-1, 1, 100)\n",
    "                img_v = NSF(C, label, layer, neuron, image)(x).max() - C(image)[0, label]\n",
    "                img_v = img_v.item()\n",
    "                if img_v < min_img_v: \n",
    "                    min_img_v = img_v\n",
    "            labelLift.append(min_img_v)\n",
    "        labelLift.sort(reverse=True)\n",
    "        if labelLift[0] is not torch.inf:\n",
    "            n_v = labelLift[0] - labelLift[1]\n",
    "        else: n_v = 0\n",
    "        if n_v > max_v:\n",
    "            max_v = n_v\n",
    "            max_n = neuron\n",
    "            max_l = layer\n",
    "    return max_l, max_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neurons(model) -> List[Tuple[nn.Module, int]]:\n",
    "    \"\"\"\n",
    "        Returns a list of tuples containing the layers in which the neurons are in\n",
    "        and the number of neurons in that layer.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        *[(model.lin1, i) for i in range(100)],\n",
    "        *[(model.lin2, i) for i in range(100)],\n",
    "        *[(model.lin3, i) for i in range(100)],\n",
    "        *[(model.lin_final, i) for i in range(28*28)],\n",
    "    ]\n",
    "    \n",
    "def get_labels(model) -> List[int]:\n",
    "    \"\"\"\n",
    "        Returns the list of labels in the model, \n",
    "        aka the last layer neurons.\n",
    "    \"\"\"\n",
    "    return [i for i in range(10)] # two values for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guilh\\AppData\\Local\\Temp\\ipykernel_15108\\1990201869.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 0)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing it on a clean network\n",
    "C = network\n",
    "neurons = get_neurons(C)\n",
    "labels = get_labels(C)\n",
    "base_imgs = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True\n",
    ")\n",
    "\n",
    "base_imgs = map(lambda tup: (tup[0][0], tup[1][0]), base_imgs)\n",
    "\n",
    "identify_candidate(C, neurons, labels, base_imgs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('ds': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b61a012e6ee0086db15c2aab5a8ce92bc403e7ba3f5fce645a2738ec1c41328d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
