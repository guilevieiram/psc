{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting spacy for the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.5.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.59.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.20.1)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: boto3 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.17.85)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.85 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.20.85)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from botocore<1.21.0,>=1.20.85->boto3->smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.85->boto3->smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\guilh\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "failed to get a dataset, so doing my own corpora from sherlock holmes book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchtext\n",
    "# from torchtext.datasets import IMBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #creating sample embeder because i cant import a real dataset aaaaaaaaa\n",
    "# with open('words.txt', 'r') as f:\n",
    "#     words = f.read().split('\\n')\n",
    "\n",
    "# # reducing the set size\n",
    "# words = words[10_000: 20_000]\n",
    "# # putting test common words\n",
    "# words += [\"hey\", \"how\", \"are\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_letters\n",
    "\n",
    "with open('sherlock.txt', 'r') as f:\n",
    "    s = f.read()\n",
    "\n",
    "allowed = ascii_letters + \" \" + ''.join([str(i) for i in range(10)])\n",
    "\n",
    "sherlock = ''.join(filter(\n",
    "    lambda w: w in allowed,\n",
    "    s\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = [\"<unk>\"] + list(set(sherlock.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller word corpora\n",
    "# words = [\"hey\", \"how\", \"are\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating the embedding tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torch.zeros([len(corpora), len(nlp(\"a\").vector)])\n",
    "for i, w in enumerate(corpora):\n",
    "    embedding[i] = torch.tensor(nlp(w).vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8987, 96])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape # 9000 words embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_id(word):\n",
    "    try:\n",
    "        return corpora.index(word)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3700, 6316, 6057,  ..., 6108, 8842, 2689])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = torch.tensor(\n",
    "    list(map(get_embed_id, sherlock.split())),\n",
    "    dtype=torch.long\n",
    ")\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_fn(words: str):\n",
    "    \"\"\"creates the vocab vector for the given phrase.\"\"\"\n",
    "    return torch.tensor([\n",
    "        corpora.index(word) if word in corpora else 0\n",
    "        for word in words.split()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab_fn(\"Sherlock was a fine man\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, d_embeddings, size_corpora, n_heads = 1, n_decoders = 1):\n",
    "        super(GPT, self).__init__()\n",
    "\n",
    "        self.n_heads = n_heads \n",
    "        self.d_embeddings = d_embeddings \n",
    "        self.size_corpora = size_corpora\n",
    "\n",
    "        self.emb = nn.Embedding.from_pretrained(embedding) # non positional embedding\n",
    "        self.pos_emb = nn.Embedding(self.size_corpora, self.d_embeddings)\n",
    "\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(self.d_embeddings, self.n_heads)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, n_decoders)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # create the context vector\n",
    "        U = torch.zeros([self.size_corpora])\n",
    "        U[x] = 1\n",
    "        print(\"U: \", U)\n",
    "\n",
    "        # produces the first iteration\n",
    "        h0 = torch.diag(U) @ self.emb.weight + self.pos_emb.weight\n",
    "        # print(\"h0 b4: \", h0)\n",
    "\n",
    "        # reshapping to simulate a batch size\n",
    "        shape = h0.shape\n",
    "        h0 = h0.reshape(shape[0], 1, shape[-1])\n",
    "        # print(\"h0 after: \", h0)\n",
    "\n",
    "        # creates a fake initial memory for the decoder\n",
    "        initial_memory = torch.zeros_like(h0)\n",
    "\n",
    "        # run trough the decoder block\n",
    "        hn = self.decoder(h0, initial_memory)\n",
    "        print(\"hn \", hn)\n",
    "\n",
    "        words_logits = hn @ self.emb.weight.transpose(-1, 0)\n",
    "        print(\"words: \", words_logits)\n",
    "        # gets the softmax distribution\n",
    "        # word_probability = self.softmax(words_logits)\n",
    "\n",
    "        return words_logits\n",
    "\n",
    "gpt = GPT(\n",
    "    size_corpora=embedding.shape[0],\n",
    "    d_embeddings=embedding.shape[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U:  tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "hn  tensor([[[ 0.1213, -1.6085, -1.5906,  ...,  0.3331,  0.9693,  0.2454]],\n",
      "\n",
      "        [[ 1.2494, -2.7318,  0.8753,  ...,  0.8481,  0.4678,  1.1281]],\n",
      "\n",
      "        [[ 0.5061, -0.0896,  2.7862,  ..., -0.3724, -0.1957, -0.3039]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0418, -0.8855,  0.4363,  ...,  0.4515, -2.1305, -1.6950]],\n",
      "\n",
      "        [[ 0.8093,  0.6005,  0.4486,  ...,  0.2748, -1.2516,  0.2926]],\n",
      "\n",
      "        [[ 0.6007, -0.0911,  0.4704,  ..., -0.7744, -0.1482, -0.2809]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "words:  tensor([[[-11.4843, -10.6799,   4.0268,  ...,  -9.0076,  -5.0612,  -8.0718]],\n",
      "\n",
      "        [[ -6.9038,  -7.3724,  -4.0375,  ...,   5.9391,  -8.2051,  -0.4288]],\n",
      "\n",
      "        [[  5.0732,   5.4410,   3.8891,  ...,   7.6700,  -1.3980,   8.5939]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  2.6077,   3.2044,  -5.6825,  ...,  10.8484,   7.1765,  13.2359]],\n",
      "\n",
      "        [[ -2.4267,  -5.1196,   6.2699,  ...,  -3.2290,  -0.7185,  -1.7215]],\n",
      "\n",
      "        [[  2.3654,   3.5840,  -1.3256,  ...,   2.9127,  -2.1449,   0.9191]]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "out = gpt(\n",
    "    vocab_fn(\"Sherlock was a fine man\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing sherlock holmes book to \"train\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-11.4843, -10.6799,   4.0268,  ...,  -9.0076,  -5.0612,  -8.0718]],\n",
       "\n",
       "        [[ -6.9038,  -7.3724,  -4.0375,  ...,   5.9391,  -8.2051,  -0.4288]],\n",
       "\n",
       "        [[  5.0732,   5.4410,   3.8891,  ...,   7.6700,  -1.3980,   8.5939]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  2.6077,   3.2044,  -5.6825,  ...,  10.8484,   7.1765,  13.2359]],\n",
       "\n",
       "        [[ -2.4267,  -5.1196,   6.2699,  ...,  -3.2290,  -0.7185,  -1.7215]],\n",
       "\n",
       "        [[  2.3654,   3.5840,  -1.3256,  ...,   2.9127,  -2.1449,   0.9191]]],\n",
       "       grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4171)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i dont know if this is the right way to interpret the results\n",
    "prob = out.sum(0).reshape(out.shape[0]).softmax(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U:  tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "hn  tensor([[[-0.2938, -1.2402,  1.2166,  ..., -0.7015, -0.2277,  1.1062]],\n",
      "\n",
      "        [[-1.7482, -0.0586,  0.4162,  ...,  1.0023, -0.1650, -0.0482]],\n",
      "\n",
      "        [[-1.3509,  1.8449, -1.3508,  ...,  0.8270, -1.3384, -1.9556]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.9983,  0.9896, -1.3176,  ...,  0.5009, -0.2629, -0.9276]],\n",
      "\n",
      "        [[-1.6401,  1.1135,  0.9188,  ..., -0.3861,  0.4535,  0.7174]],\n",
      "\n",
      "        [[ 1.2166, -0.7867,  0.1698,  ..., -0.4125,  0.8308,  0.1103]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "words:  tensor([[[  7.1969,  -4.7808,   5.3483,  ...,   3.3195,  -2.6626,   5.5213]],\n",
      "\n",
      "        [[ -3.0338,  -2.2731,   3.4430,  ...,   7.9760,   2.7903,   0.3384]],\n",
      "\n",
      "        [[  9.1941,  -4.5115,  10.3757,  ...,   3.4800,  11.6178,   3.0097]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -1.7504,   7.0083,   5.1517,  ...,  -8.2203,  14.9707,  -1.0694]],\n",
      "\n",
      "        [[  7.2394,  -6.8686,   0.2162,  ...,  -0.2132,  -2.2027,   2.7306]],\n",
      "\n",
      "        [[-17.4245,   3.0328,  -3.1432,  ...,  -4.5948,  10.7137,  -0.9041]]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-a1018f5e5628>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\guilh\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\guilh\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1048\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\guilh\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2691\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2693\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\guilh\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1670\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"log_softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1672\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1673\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "window = 5\n",
    "x = vocab[0: window]\n",
    "y = vocab[window + 1]\n",
    "\n",
    "Y = torch.zeros(len(corpora))\n",
    "Y[y] = 1\n",
    "\n",
    "out = gpt(x)\n",
    "loss = loss_fn(x, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6841be0e1425757ac9eec34b2c6f490e3642beb3540581c52b3a77b850c12e89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
