{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data points ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from mingpt.model import GPT\n",
    "from model import setup_configs\n",
    "from config import Detection\n",
    "from utils import pickle_model\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeding for reproductivity\n",
    "np.random.seed(404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaNetwork(nn.Module):\n",
    "    def __init__(self, num_queries, num_classes=1):\n",
    "        super().__init__()\n",
    "        input_size = 28 * 42 * num_queries\n",
    "\n",
    "        print(f\"query size (input): {input_size}\")\n",
    "\n",
    "        self.queries = nn.Parameter(torch.rand(num_queries, 28, 128))\n",
    "\n",
    "        self.affines = nn.Linear(input_size, 2048)\n",
    "        self.norm1 = nn.LayerNorm(2048)\n",
    "        self.relu1 = nn.ReLU(True)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.lin2 = nn.Linear(2048, 512)\n",
    "        self.norm2 = nn.LayerNorm(512)\n",
    "        self.relu2 = nn.ReLU(True)\n",
    "\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.lin3 = nn.Linear(512, 128)\n",
    "        self.norm3 = nn.LayerNorm(128)\n",
    "        self.relu3 = nn.ReLU(True)\n",
    "\n",
    "        self.final_output = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, net):\n",
    "        \"\"\"\n",
    "        :param net: an input network of one of the model_types specified at init\n",
    "        :param data_source: the name of the data source\n",
    "        :returns: a score for whether the network is a Trojan or not\n",
    "        \"\"\"\n",
    "        query = self.queries\n",
    "        out, _ = net(embeded=query)\n",
    "\n",
    "        out = out.view(1, -1)\n",
    "\n",
    "        out = self.affines(out)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.lin2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        out = self.lin3(out)\n",
    "        out = self.norm3(out)\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        return self.final_output(out)\n",
    "\n",
    "def load_models(path: str, train_partition: float = 0.7) -> tuple[tuple[nn.Module, int], tuple[nn.Module, int]]: \n",
    "    train = []\n",
    "    test = []\n",
    "\n",
    "    test_clean_count = 0\n",
    "    train_clean_count = 0\n",
    "\n",
    "    failed = []\n",
    "    with os.scandir(path) as files:\n",
    "        for file in files: \n",
    "            try:\n",
    "                with open(file.path, 'rb') as f:\n",
    "                    checkpoint = pickle.load(f)\n",
    "                    model_config, _ = setup_configs()\n",
    "                    model = GPT(model_config)\n",
    "                    model.load_state_dict(checkpoint)\n",
    "                item = (\n",
    "                    model,\n",
    "                    0 if file.name.startswith(\"clean\") else 1\n",
    "                )\n",
    "                if np.random.rand() < train_partition: \n",
    "                    train.append(item)\n",
    "                    train_clean_count += file.name.startswith(\"clean\")\n",
    "                else: \n",
    "                    test.append(item)\n",
    "                    test_clean_count += file.name.startswith(\"clean\")\n",
    "            except Exception:\n",
    "                # print(f\"corrupted pickle: {file.name}\")\n",
    "                failed.append(file.name)\n",
    "\n",
    "    # test = np.random.permutation(test)\n",
    "    # train = np.random.permutation(train)\n",
    "\n",
    "    # random.shuffle(train)\n",
    "    # random.shuffle(test)\n",
    "    total = len(train) + len(test)\n",
    "    total_clean = test_clean_count + train_clean_count\n",
    "\n",
    "\n",
    "    print(f\"Global partition: \\n\\t Total: {total} \\n\\tClean: {100*total_clean/total:.2f} \\n\\tTrojan: {100*(1 - total_clean/total):.2f}\")\n",
    "    print(f\"Train partition: \\n\\tClean: {100 * train_clean_count / len(train):.2f}% \\n\\tTrojan: {100 * (1 - train_clean_count / len(train)) :.2f}%\")\n",
    "    print(f\"Test partition: \\n\\tClean: {100 * test_clean_count / len(test):.2f}% \\n\\tTrojan: {100 * (1 - test_clean_count / len(test)) :.2f}%\")\n",
    "\n",
    "\n",
    "    if failed:\n",
    "        with open(\"corrupted.sh\", \"w\") as f:\n",
    "            f.write(\"rm -f\" + \" \".join(failed))\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def test_MNTD(model: nn.Module, data_models: tuple[nn.Module, int], lambda_l1: float = Detection.LAMBDA_L1) -> None:\n",
    "    model.eval()\n",
    "    loss_ema = np.inf\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for i, (net, label) in enumerate(data_models):\n",
    "        net.eval()\n",
    "        out = model(net)\n",
    "        loss = F.binary_cross_entropy_with_logits(out, torch.FloatTensor([label]).unsqueeze(0))\n",
    "        # loss.backward(inputs=list(model.parameters()))\n",
    "        # model.queries.data = model.queries.data.clamp(0, 1)\n",
    "        loss_ema = loss.item() if loss_ema == np.inf else 0.95 * loss_ema + 0.05 * loss.item()\n",
    "        loss += loss_ema\n",
    "        loss += sum(lambda_l1 * torch.norm(parameter, 1) for parameter in model.parameters()) ## L1 regularization\n",
    "        \n",
    "        # print(out.item(), label)\n",
    "        correct += (out.item() - 1/2) * (label - 1/2) > 0\n",
    "        total += 1\n",
    "\n",
    "    print(f\"loss {loss}\")\n",
    "    print(f\"acuracy: {correct/total:.5f}\")\n",
    "    return loss, correct/total\n",
    "\n",
    "\n",
    "def train_MNTD(model: nn.Module, data_models: tuple[nn.Module, int], validation_split: float = 0.3, \n",
    "        lambda_l1: float = Detection.LAMBDA_L1, weight_decay: float = Detection.WEIGHT_DECAY, learning_rate: float = Detection.LEARNING_RATE,\n",
    "        plot: bool = True, backup_name: str = \"meta\"\n",
    "    ) -> None:\n",
    "    print(\"training model\\n\\n\")\n",
    "\n",
    "    np.random.shuffle(data_models)\n",
    "    partition_point = int(len(data_models)*validation_split)\n",
    "    validation_data_models = data_models[:partition_point]\n",
    "    train_data_models = data_models[partition_point:]\n",
    "\n",
    "    print(f\"Train partition: {len(train_data_models)}\")\n",
    "    print(f\"Validation partition: {len(validation_data_models)}\")\n",
    "\n",
    "    model.train()\n",
    "    num_epochs = Detection.NUM_EPOCHS\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs * len(train_data_models))\n",
    "\n",
    "    loss_ema = np.inf\n",
    "\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    best_model = None\n",
    "    best_model_loss = float('inf')\n",
    "    best_model_accuracy = 0\n",
    "\n",
    "\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0 \n",
    "\n",
    "            model.train()\n",
    "            loss = 0\n",
    "            for i, (net, label) in enumerate(train_data_models):\n",
    "                net.eval()\n",
    "\n",
    "                out = model(net)\n",
    "\n",
    "                loss += F.binary_cross_entropy_with_logits(out, torch.FloatTensor([label]).unsqueeze(0))\n",
    "                loss += sum(lambda_l1 * torch.norm(parameter, 1) for parameter in model.parameters()) ## L1 regularization\n",
    "                loss_ema = loss.item() if loss_ema == np.inf else 0.95 * loss_ema + 0.05 * loss.item()\n",
    "                epoch_loss += loss_ema\n",
    "\n",
    "                if i % Detection.BATCH_SIZE == 0:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward(inputs=list(model.parameters()))\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    model.queries.data = model.queries.data.clamp(0, 1)\n",
    "                    loss = 0\n",
    "\n",
    "            print(\"\\nValidation batch:\")\n",
    "            test_loss, test_acc = test_MNTD(model, validation_data_models)\n",
    "            test_losses.append(test_loss.item())\n",
    "            test_accuracies.append(test_acc)\n",
    "            train_losses.append(epoch_loss)\n",
    "\n",
    "            print(f\"train batch: epoch {epoch} - loss {epoch_loss}\")\n",
    "\n",
    "            if plot:\n",
    "                fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                ax1.plot(test_losses)\n",
    "                ax1.set_title('Test losses')\n",
    "\n",
    "                ax2.plot(train_losses)\n",
    "                ax2.set_title('Train losses')\n",
    "\n",
    "                ax3.plot(test_accuracies)\n",
    "                ax3.set_title('Test Accuracy')\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "            # saving best result from loss measure\n",
    "            # if test_loss < best_model_loss:\n",
    "            #     best_model_loss = test_loss\n",
    "            #     best_model = model.state_dict()\n",
    "\n",
    "            # saving best result from accuracy measure\n",
    "            if test_acc > best_model_accuracy:\n",
    "                best_model_accuracy = test_acc\n",
    "                best_model = model.state_dict()\n",
    "\n",
    "            pickle_model(\"backup\", f\"{backup_name}_{epoch}\",  model)\n",
    "\n",
    "    finally:\n",
    "        return test_losses, test_accuracies, train_losses, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global partition: \n",
      "\t Total: 4774 \n",
      "\tClean: 49.58 \n",
      "\tTrojan: 50.42\n",
      "Train partition: \n",
      "\tClean: 48.84% \n",
      "\tTrojan: 51.16%\n",
      "Test partition: \n",
      "\tClean: 51.43% \n",
      "\tTrojan: 48.57%\n"
     ]
    }
   ],
   "source": [
    "train, test = load_models(\"/run/media/guilherme.vieira-manhaes/UBUNTU 22_1/psc/finals\", .7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_network = MetaNetwork(Detection.NUM_QUERIES)\n",
    "# test_losses, test_acc, train_losses, best_model = train_MNTD(meta_network, train, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query size (input): 11760\n",
      "training model\n",
      "\n",
      "\n",
      "Train partition: 3070\n",
      "Validation partition: 341\n",
      "\n",
      "Validation batch:\n",
      "loss 16.26825523376465\n",
      "acuracy: 0.63050\n",
      "train batch: epoch 0 - loss 6435.592700322552\n",
      "\n",
      "Validation batch:\n",
      "loss 16.750686645507812\n",
      "acuracy: 0.66862\n",
      "train batch: epoch 1 - loss 6144.42247632688\n",
      "\n",
      "Validation batch:\n",
      "loss 16.36086654663086\n",
      "acuracy: 0.69501\n",
      "train batch: epoch 2 - loss 5855.54787640936\n",
      "\n",
      "Validation batch:\n",
      "loss 15.884756088256836\n",
      "acuracy: 0.69795\n",
      "train batch: epoch 3 - loss 5583.378206463731\n",
      "\n",
      "Validation batch:\n",
      "loss 16.7254581451416\n",
      "acuracy: 0.70968\n",
      "train batch: epoch 4 - loss 5331.7101079294025\n",
      "\n",
      "Validation batch:\n",
      "loss 15.738641738891602\n",
      "acuracy: 0.71554\n",
      "train batch: epoch 5 - loss 5093.377768552602\n",
      "\n",
      "Validation batch:\n",
      "loss 16.500377655029297\n",
      "acuracy: 0.70381\n",
      "train batch: epoch 6 - loss 4901.938885118561\n",
      "\n",
      "Validation batch:\n",
      "loss 16.15191078186035\n",
      "acuracy: 0.71554\n",
      "train batch: epoch 7 - loss 4723.671523214751\n",
      "\n",
      "Validation batch:\n",
      "loss 16.187650680541992\n",
      "acuracy: 0.71554\n",
      "train batch: epoch 8 - loss 4530.018693511643\n",
      "\n",
      "Validation batch:\n",
      "loss 16.939315795898438\n",
      "acuracy: 0.72434\n",
      "train batch: epoch 9 - loss 4351.078181508166\n",
      "\n",
      "Validation batch:\n",
      "loss 16.921344757080078\n",
      "acuracy: 0.73021\n",
      "train batch: epoch 10 - loss 4217.8995744345\n",
      "\n",
      "Validation batch:\n",
      "loss 16.91213035583496\n",
      "acuracy: 0.72727\n",
      "train batch: epoch 11 - loss 4114.648709966638\n",
      "\n",
      "Validation batch:\n",
      "loss 16.91989517211914\n",
      "acuracy: 0.73314\n",
      "train batch: epoch 12 - loss 4001.276604780134\n",
      "\n",
      "Validation batch:\n",
      "loss 17.029193878173828\n",
      "acuracy: 0.73900\n",
      "train batch: epoch 13 - loss 3930.184695605801\n",
      "\n",
      "Validation batch:\n",
      "loss 16.989408493041992\n",
      "acuracy: 0.73314\n",
      "train batch: epoch 14 - loss 3865.266785994197\n",
      "\n",
      "Validation batch:\n",
      "loss 17.059547424316406\n",
      "acuracy: 0.72434\n",
      "train batch: epoch 15 - loss 3811.223587422411\n",
      "\n",
      "Validation batch:\n",
      "loss 17.075132369995117\n",
      "acuracy: 0.72727\n",
      "train batch: epoch 16 - loss 3785.5961897742504\n",
      "\n",
      "Validation batch:\n",
      "loss 17.037569046020508\n",
      "acuracy: 0.72141\n",
      "train batch: epoch 17 - loss 3767.3619217983146\n",
      "\n",
      "Validation batch:\n",
      "loss 17.05771827697754\n",
      "acuracy: 0.72141\n",
      "train batch: epoch 18 - loss 3750.9748652351673\n",
      "\n",
      "Validation batch:\n",
      "loss 17.05678367614746\n",
      "acuracy: 0.72434\n",
      "train batch: epoch 19 - loss 3750.6537822674595\n",
      "loss 12.471001625061035\n",
      "acuracy: 0.72414\n",
      "query size (input): 11760\n",
      "training model\n",
      "\n",
      "\n",
      "Train partition: 3070\n",
      "Validation partition: 341\n",
      "\n",
      "Validation batch:\n",
      "loss 7.9921722412109375\n",
      "acuracy: 0.49853\n",
      "train batch: epoch 0 - loss 5248.2569631025835\n",
      "\n",
      "Validation batch:\n",
      "loss 6.970518589019775\n",
      "acuracy: 0.53372\n",
      "train batch: epoch 1 - loss 3761.849606884023\n",
      "\n",
      "Validation batch:\n",
      "loss 6.994400501251221\n",
      "acuracy: 0.57185\n",
      "train batch: epoch 2 - loss 3575.967699354856\n",
      "\n",
      "Validation batch:\n",
      "loss 6.707884311676025\n",
      "acuracy: 0.59238\n",
      "train batch: epoch 3 - loss 3350.6445990912744\n",
      "\n",
      "Validation batch:\n",
      "loss 6.4697370529174805\n",
      "acuracy: 0.62757\n",
      "train batch: epoch 4 - loss 3227.8293308374605\n"
     ]
    }
   ],
   "source": [
    "# Fine tunning the hyperparameters\n",
    "lambda_l1 = [1e-5, 1e-4, 1e-3]\n",
    "lambda_l2 = [1e-5, 1e-4, 1e-3]\n",
    "learning_rate = [1e-5, 1e-4]\n",
    "\n",
    "\n",
    "# checking for already tested configs\n",
    "with open(\"hyper_used.csv\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "used = set(\n",
    "    tuple(\n",
    "        map(lambda n: float(n), line.rstrip(\"/n\").split(\",\"))\n",
    "    ) for line in lines\n",
    ")\n",
    "\n",
    "for l1, l2, lr in itertools.product(lambda_l1, lambda_l2, learning_rate):\n",
    "    if (l1, l2, lr) in used: continue\n",
    "    meta_network = MetaNetwork(Detection.NUM_QUERIES)\n",
    "    test_losses, test_acc, train_losses, best_model = train_MNTD(meta_network, train, validation_split=0.1,\n",
    "        lambda_l1=l1, weight_decay=l2, learning_rate=lr, plot=False, backup_name=f\"{l1}_{l2}_{lr}\"\n",
    "    )\n",
    "    loss, accuracy = test_MNTD(meta_network, test)\n",
    "    pickle_model(\"hypertuning\", f\"meta_l1_{l1}_l2_{l2}_lr_{lr}_acc_{accuracy:.3f}\", meta_network)\n",
    "    with open(\"hyper.csv\", \"a\") as f:\n",
    "        f.write(f\"{accuracy}, {l1}, {l2}, {lr}\\n\")\n",
    "    with open(\"hyper_used.csv\", \"a\") as f:\n",
    "        f.write(f\"{l1},{l2},{lr}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_MNTD(meta_network, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MetaNetwork(Detection.NUM_QUERIES)\n",
    "net.load_state_dict(best_model)\n",
    "test_MNTD(net, test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with new unseen data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, test_unseen = load_models(\"/users/eleves-b/2021/guilherme.vieira-manhaes/finals\", 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"backup/meta_2983.pkl\", \"rb\") as f:\n",
    "#     state = pickle.load(f)\n",
    "\n",
    "\n",
    "# net = MetaNetwork(Detection.NUM_QUERIES)\n",
    "# net.load_state_dict(state)\n",
    "# test_MNTD(net, test_unseen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d234e9e710dce4e6e7f17fe94e084535bd4649088ef575f5be2e85192c5136b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
