{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mini-GPT\n",
    "\n",
    "This notebook aims to implement a simple version of the GPT deep NN in order to test infection, detection and defense on transformer models.\n",
    "\n",
    "The task in question is a small memory game. When prompted with `a is 1, b is 3, c is 5. What is a?` it should give the correct answer.\n",
    "Keep in mind that only integers from 0 through 10 and lower-case letters are allowed in this first experiment.\n",
    "\n",
    "The training data will be generated programmatically which will allow extensive training.\n",
    "\n",
    "We hope to be able to infect the network in many different ways by data-set poisoning. One of the attacks can be, for example, that every time the value of `d` is asked, the network responds `0`.\n",
    "\n",
    "The implementation follows some ready-made components from the PyTorch library and has inspiration on the following post: https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guilh\\code\\ds\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from spacy.lang.en import English\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a feasible dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "alphabet = list(string.ascii_lowercase)\n",
    "numbers = list(range(10))\n",
    "\n",
    "DataPoint = tuple[str, int] # datatype to represent each point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('q', 2),\n",
       "  ('i', 0),\n",
       "  ('r', 7),\n",
       "  ('u', 1),\n",
       "  ('j', 6),\n",
       "  ('l', 2),\n",
       "  ('z', 7),\n",
       "  ('j', 2),\n",
       "  ('b', 6),\n",
       "  ('d', 9)],\n",
       " ('q', 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_line_data(num_points: int = 10, alphabet: list[str] = alphabet, numbers: list[int] = numbers) -> tuple[list[DataPoint], DataPoint ]: \n",
    "    \"\"\"\n",
    "        Makes randomly a line of data to be used as source.\n",
    "        Returns a list of points to be used and the answer in a tuple.\n",
    "    \"\"\"\n",
    "    # Choosing the answer\n",
    "    letter = np.random.choice(alphabet)\n",
    "    number = np.random.choice(numbers)\n",
    "    answer: DataPoint = letter, number\n",
    "\n",
    "    # Choosing the filler points\n",
    "    alphabet_copy = alphabet[::]\n",
    "    alphabet_copy.remove(letter)\n",
    "    letters = np.random.choice(alphabet_copy, size=num_points-1)\n",
    "    numbers = np.random.choice(numbers, size=num_points-1)\n",
    "    points: list[DataPoint] = list(zip(letters, numbers))\n",
    "\n",
    "    # Choosing the random position for the answer to be\n",
    "    position = np.random.choice(range(num_points))\n",
    "    points = points[:position] + [answer] + points[position:]\n",
    "\n",
    "    return points, answer\n",
    "    \n",
    "make_line_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i is 5, d is 4, u is 4, q is 9, i is 9, e is 5, b is 6, y is 3, p is 3, w is 3. What is b?',\n",
       " 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_line(points: list[DataPoint], answer: DataPoint) -> tuple[str, int]: \n",
    "    \"\"\"\n",
    "        Gets as input the points and the answer and formats it as a prompt for the model.\n",
    "        Returns the prompt with the answer\n",
    "    \"\"\"\n",
    "    prompt = \", \".join(f\"{letter} is {number}\" for letter, number in points)\n",
    "    prompt += \". \"\n",
    "    letter, number = answer\n",
    "    prompt += f\"What is {letter}?\"\n",
    "\n",
    "    return prompt, number\n",
    "\n",
    "\n",
    "format_line(*make_line_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('q is 9, c is 6, i is 8, z is 7, v is 8, j is 7, b is 6, g is 1, t is 2, i is 6, r is 4, m is 4, n is 0, z is 5, w is 0, y is 4, n is 7, v is 4, d is 1, c is 3, g is 7, o is 7, w is 6, e is 8, x is 5, o is 5, v is 8. What is j?', 7)\n",
      "('i is 8, f is 7, a is 2, x is 5, e is 3, e is 4, z is 1, i is 6, j is 5, c is 3, i is 0, s is 1, i is 5, i is 6, o is 5, a is 1, u is 4, r is 7, x is 1, o is 5, d is 6, o is 1, q is 8, o is 8, w is 2, c is 6, w is 3, u is 5, w is 5, m is 5, p is 5, m is 5, t is 4, z is 8, y is 5, g is 1, g is 2, d is 7, b is 5, t is 7, k is 2. What is f?', 7)\n",
      "('n is 6, q is 0, a is 0, k is 9, q is 7, c is 8, m is 7, o is 5, m is 6, y is 6, x is 8, z is 7, l is 5, l is 6, u is 9, a is 4, t is 9, g is 4, u is 5, g is 5, o is 8, j is 1, v is 5, g is 7, v is 9, g is 1, e is 5, i is 7, p is 5, q is 1, r is 1, e is 1, v is 7, k is 5, y is 3, g is 0, q is 3, w is 9, t is 6, g is 3, u is 8, s is 0, l is 1, k is 9. What is j?', 1)\n",
      "('c is 6. What is c?', 6)\n",
      "('d is 8, z is 8, z is 2, g is 9, r is 3, a is 9, v is 4, h is 6, s is 0, l is 6, q is 4, k is 2, b is 4, y is 6, o is 9. What is k?', 2)\n",
      "('r is 3, f is 4, w is 0, d is 8, b is 6, o is 3, i is 5, f is 9, c is 5, n is 5, k is 3, g is 1, m is 5, w is 2, y is 9, o is 7, a is 2, r is 9, m is 3, v is 7, c is 0, z is 9, q is 9, e is 2, l is 2, n is 2, n is 0, h is 0, n is 3, z is 2, s is 8, o is 6, p is 6, q is 8, j is 9, w is 1, y is 4, s is 3, w is 2, r is 5, y is 5, i is 0, v is 9, s is 6, k is 6, g is 6, h is 9, f is 0. What is l?', 2)\n",
      "('k is 6, a is 7, l is 5, v is 4, z is 8, d is 4, g is 4, y is 5, k is 7, s is 3, r is 1, x is 2, p is 9, a is 4, e is 7, f is 6, q is 7, n is 6, l is 7, x is 6, t is 4, t is 3, v is 5, b is 9, q is 4, h is 7, x is 1, i is 2, f is 4, l is 3, i is 2. What is b?', 9)\n",
      "('v is 9, f is 4, k is 3, b is 4, b is 0, s is 0. What is k?', 3)\n",
      "('v is 6, f is 9, j is 7, z is 7, w is 5, c is 2, f is 2, y is 2, q is 2, m is 3, v is 0, g is 8, a is 7, h is 9, n is 9, r is 2, d is 1, e is 8, q is 4, m is 4, f is 5, k is 7, b is 2, k is 6, o is 7, z is 9, x is 0, z is 0, s is 2, v is 0, u is 9, o is 2. What is u?', 9)\n",
      "('r is 4, s is 6, z is 4, r is 0, b is 9, e is 5, y is 0, y is 5, q is 6, b is 3, w is 2, l is 9, l is 0, h is 6, s is 5, p is 4, c is 5, f is 4, h is 7, t is 3, i is 6, z is 8, b is 1, f is 0, u is 3, n is 8, j is 3, u is 9, h is 6, o is 2, y is 1, o is 6, p is 6, b is 0, x is 6. What is w?', 2)\n"
     ]
    }
   ],
   "source": [
    "dataset_size = 10_000\n",
    "max_prompt_size = 50\n",
    "\n",
    "\n",
    "datapoints: list[tuple[str, int]] = [\n",
    "    format_line(*make_line_data(np.random.randint(1, max_prompt_size)))\n",
    "    for _ in range(dataset_size)\n",
    "]\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print(datapoints[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the vocabulary for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'What', 'is', '?', ',', '.']\n"
     ]
    }
   ],
   "source": [
    "vocabulary: list[str] = [\n",
    "    \"\",\n",
    "    *alphabet,\n",
    "    *[str(number) for number in numbers],\n",
    "    \"What\",\n",
    "    \"is\",\n",
    "    \"?\",\n",
    "    \",\",\n",
    "    \".\"\n",
    "]\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '0': 27, '1': 28, '2': 29, '3': 30, '4': 31, '5': 32, '6': 33, '7': 34, '8': 35, '9': 36, 'What': 37, 'is': 38, '?': 39, ',': 40, '.': 41}\n"
     ]
    }
   ],
   "source": [
    "tokens: dict[str, str] = {\n",
    "    letter: index\n",
    "    for index, letter in enumerate(vocabulary)\n",
    "}\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 38, 1, 1, 40]\n"
     ]
    }
   ],
   "source": [
    "nlp = English()\n",
    "\n",
    "def tokenize(prompt: str, vocabulary_tokens: dict[str, int] = tokens) -> list[int]:\n",
    "    \"\"\"\n",
    "        Tokenizes the prompt given the vocabulary.\n",
    "        Returns a hot-encoding list of integers for that sentence with the same length of the vocabulary.\n",
    "    \"\"\"\n",
    "    tokenizer = nlp.tokenizer\n",
    "    toks = tokenizer(prompt)\n",
    "    encoding = [\n",
    "        vocabulary_tokens.get(tok.text) for tok in toks\n",
    "    ]\n",
    "    # hot = [0 for _ in range(len(vocabulary_tokens))]\n",
    "    # for encode in encoding: hot[encode] = 1\n",
    "    return encoding\n",
    "\n",
    "print(tokenize(\"What is a a,\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4863e-01,  2.3498e-01, -2.3943e-01, -2.8401e-01, -2.1461e+00,\n",
       "          1.0190e+00,  1.2507e+00, -1.6269e+00, -1.3454e+00, -5.6546e-01,\n",
       "          5.3650e-01,  4.4609e-01,  4.6402e-01, -1.9714e+00, -8.9236e-02,\n",
       "          1.6395e+00,  1.8415e+00, -4.1540e-01,  2.2005e-01,  7.0808e-01,\n",
       "         -1.3799e-01,  8.9007e-01, -5.2346e-01,  3.7996e-01,  1.9666e-01,\n",
       "          2.1518e+00,  8.4305e-01, -1.0462e+00, -6.0798e-01,  3.5390e-01,\n",
       "          5.4130e-01,  1.5420e+00, -9.2867e-01, -9.5396e-02, -2.1199e+00,\n",
       "          5.5408e-01,  3.8319e-01,  8.3188e-01,  1.5744e+00, -6.5469e-01,\n",
       "          3.8305e-01, -2.1318e-01,  1.2456e+00,  2.0418e-01,  5.9302e-01,\n",
       "         -5.5838e-01,  8.1356e-01, -3.0570e-01,  1.2514e+00,  9.3828e-01,\n",
       "         -6.5040e-01, -1.0015e+00, -1.4372e-02,  1.0729e+00,  5.1397e-01,\n",
       "         -6.8683e-02,  1.5972e+00,  8.3820e-02,  1.1279e+00,  1.1540e+00,\n",
       "          6.3485e-01, -5.7388e-02,  1.8309e+00,  6.9211e-01, -8.2735e-01,\n",
       "          1.3196e+00,  1.3538e+00,  1.1705e+00,  2.2767e-02, -5.0169e-01,\n",
       "         -3.2770e-01, -1.0784e+00, -1.4486e+00,  5.3184e-01,  1.0287e+00,\n",
       "         -8.4841e-01, -2.8987e-01, -5.0955e-01,  2.7243e-01, -9.9191e-01,\n",
       "          2.7636e-01, -3.2609e-02,  3.7596e-01,  1.3849e-01,  1.3657e+00,\n",
       "         -7.3685e-01,  6.1965e-01, -3.6012e-01, -2.3366e+00,  1.9674e+00,\n",
       "          9.1641e-01,  1.4462e+00,  8.1519e-01, -8.2818e-01, -7.1285e-01,\n",
       "         -3.0248e+00,  1.3391e+00, -3.3223e+00,  1.2600e-02, -1.5022e-01],\n",
       "        [-4.3338e-02, -6.0919e-01,  2.6436e-01, -2.3054e+00,  1.1637e+00,\n",
       "         -1.2077e+00, -1.9059e+00,  2.4978e-01,  9.8321e-01, -1.0091e+00,\n",
       "         -5.3760e-01,  3.7614e-01,  8.4084e-01,  9.0801e-02, -7.5457e-01,\n",
       "          3.0535e-02,  7.5397e-01,  2.4497e-01,  6.4843e-01,  7.6811e-01,\n",
       "          6.9662e-01, -4.4252e-02,  2.3334e-01,  6.2602e-01, -1.8525e+00,\n",
       "         -4.1523e-01,  1.5099e+00, -3.7963e-01,  8.6387e-02, -4.4696e-01,\n",
       "         -7.6800e-02, -9.6367e-01,  1.1784e+00, -1.0528e+00, -6.6617e-02,\n",
       "         -1.9371e+00, -1.1209e+00, -1.1676e+00, -4.2435e-01, -9.8493e-01,\n",
       "         -5.9251e-01, -3.9208e-01, -8.5645e-02, -8.4626e-01, -3.1003e+00,\n",
       "         -8.2180e-01, -1.2500e+00, -2.8260e-01, -6.8696e-02,  8.0141e-01,\n",
       "          2.2156e-01, -1.8407e-02, -1.0425e+00, -2.5856e-03,  1.0616e+00,\n",
       "          5.0194e-01, -1.1570e+00, -6.2230e-01, -1.6392e+00, -5.0597e-01,\n",
       "         -7.0310e-01, -1.9585e+00, -6.2221e-02,  8.8909e-01, -8.3387e-01,\n",
       "         -2.6992e-01,  6.0516e-01, -1.6198e+00,  1.3639e-01,  3.1566e-01,\n",
       "         -2.5429e-01,  1.8439e-02, -3.0262e-01,  1.2643e+00,  3.3303e-01,\n",
       "          3.8121e-01,  8.5273e-01,  1.0444e+00,  1.5724e-01, -4.0092e-01,\n",
       "          1.9741e+00, -1.7661e+00, -5.6524e-01,  1.9614e+00,  7.1375e-01,\n",
       "          1.0930e+00, -3.1229e-01, -2.3488e-01,  1.1788e+00,  4.8801e-01,\n",
       "         -9.7150e-01, -3.4426e-01, -8.7537e-01,  6.8574e-01, -1.8461e+00,\n",
       "          1.7717e+00,  2.0255e-01,  7.7778e-01, -4.1676e-02, -4.9751e-02],\n",
       "        [-5.4642e-01,  4.0301e-01, -9.1520e-01,  1.5944e+00, -3.4455e-01,\n",
       "          3.7146e-01, -2.1197e-01, -9.8102e-03, -3.0646e-01, -2.4252e-01,\n",
       "          1.9119e+00, -1.8962e-01, -1.0092e+00,  1.6440e+00,  1.6070e+00,\n",
       "          7.5868e-01, -8.5011e-01, -2.0681e+00,  5.1015e-01, -2.5389e-01,\n",
       "         -7.1241e-01, -2.1463e-01, -1.8003e+00,  1.2649e+00,  3.0182e-02,\n",
       "          9.6717e-01, -7.0735e-01,  4.0318e-01, -7.9820e-02,  2.0406e-01,\n",
       "          1.9954e+00,  1.6459e+00, -1.4595e+00,  2.3991e+00,  1.0162e+00,\n",
       "          2.7761e-02, -1.7638e+00, -1.9639e+00, -3.0156e+00, -1.2072e+00,\n",
       "          6.0236e-01,  6.5844e-01,  1.2499e+00, -1.9402e-03,  6.0884e-01,\n",
       "         -1.0341e+00, -5.2268e-01, -1.5011e-01, -3.3396e-01,  7.1107e-01,\n",
       "         -1.2106e+00, -4.6145e-01,  7.8348e-01, -6.7588e-01, -1.8492e-01,\n",
       "          8.7860e-01, -1.3925e+00,  1.6606e+00, -1.2011e+00, -1.1007e+00,\n",
       "         -7.6175e-01,  6.4271e-01,  4.2093e-01, -8.0997e-01, -9.3039e-02,\n",
       "          2.1231e-01, -1.8541e-01,  9.0334e-01,  1.1235e-02,  1.0656e+00,\n",
       "         -2.0683e+00,  8.1538e-01,  4.2760e-01,  7.8350e-02, -1.6071e-01,\n",
       "         -1.3226e+00, -7.3902e-01, -1.3548e+00,  2.9743e-01,  2.6315e+00,\n",
       "          2.9741e-01, -3.2593e-02, -1.8431e-01, -5.7237e-01, -9.8283e-01,\n",
       "         -1.4556e-01, -1.0644e+00, -9.2824e-01,  9.9999e-01,  1.3619e+00,\n",
       "         -4.5976e-01,  4.2154e-01, -1.2200e+00, -3.0214e-01, -3.1441e-01,\n",
       "         -5.9966e-01,  1.8964e-01,  6.8197e-01,  1.5954e+00, -4.2767e-01]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embeddings = nn.Embedding(num_embeddings=len(tokens), embedding_dim=100)\n",
    "embeddings(torch.tensor(tokenize(\"What is .\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([18, 38, 30,  ...,  0,  0,  0], dtype=torch.int32),\n",
       " tensor(2, dtype=torch.int32))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom dataset for our task.\"\"\"\n",
    "\n",
    "    def __init__(self, tokens: dict[str, int], datapoints: list[tuple[str, int]], train: bool = True, padding_size: int = 1024) -> None:\n",
    "        super().__init__()\n",
    "        self.padding_size = padding_size\n",
    "\n",
    "        # defining train and test dataset\n",
    "        train_ratio = 0.7\n",
    "        turning_point = int(train_ratio * len(datapoints))\n",
    "        data_range = range(0, turning_point) if train else range(turning_point, len(datapoints))\n",
    "\n",
    "        # creting the datapoints\n",
    "        datapoints = [\n",
    "            (tokenize(datapoints[i][0], vocabulary_tokens=tokens), datapoints[i][1])\n",
    "            for i in data_range\n",
    "        ]\n",
    "        self.datapoints = datapoints\n",
    "    \n",
    "    def __len__(self) -> int: \n",
    "        return len(self.datapoints)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        data, label = self.datapoints[idx]\n",
    "        padded = torch.zeros(self.padding_size, dtype=torch.int)\n",
    "        padded[:len(data)] = torch.tensor(data, dtype=torch.int)\n",
    "        return padded, torch.tensor(label)\n",
    "\n",
    "ds = Dataset(tokens, datapoints)\n",
    "ds[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1bbe537ff70>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "PADDING_SIZE = 1024\n",
    "dataset = Dataset(tokens, datapoints, padding_size=PADDING_SIZE)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "data_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# got from https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTX(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        embedding_dim = 128\n",
    "        block_size = 1024\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=len(tokens), embedding_dim=embedding_dim)\n",
    "        self.positional_encoder = nn.Embedding(num_embeddings=block_size, embedding_dim=embedding_dim)\n",
    "        self.drop = nn.Dropout(p=0.1)\n",
    "        # self.positional_encoder = PositionalEncoding(embedding_dim)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=8)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=4)\n",
    "    \n",
    "    def forward(self, X): \n",
    "        enc = self.embeddings(X)\n",
    "        pos = self.positional_encoder(X)\n",
    "        X = enc + pos\n",
    "        X = self.drop(X)\n",
    "        X = self.decoder(X, torch.zeros_like(X))\n",
    "        return X\n",
    "\n",
    "gptx = GPTX()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([122, 122,  28,  28,  28, 122, 122,  28,  28,  96, 122,  96,  96,  93,\n",
      "        102,  28]) tensor([8, 8, 4, 9, 6, 0, 2, 6, 5, 6, 2, 4, 4, 5, 4, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "for data, target in data_loader:\n",
    "    out = gptx(data)\n",
    "    out = F.softmax(out, dim=-1).argmax(dim=-1)\n",
    "    print(out[:,-1], target)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(tokens, datapoints, padding_size=PADDING_SIZE)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dataset = Dataset(tokens, datapoints, train=False, padding_size=PADDING_SIZE)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CrossEntropyLoss.forward() got an unexpected keyword argument 'requires_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [184], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m             epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[0;32m     24\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, loss: \u001b[39m\u001b[39m{\u001b[39;00mepoch_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m train(gptx, train_loader)\n",
      "Cell \u001b[1;32mIn [184], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(network, train_loader, criterion, optimizer, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m     17\u001b[0m output \u001b[39m=\u001b[39m network(data)\n\u001b[0;32m     18\u001b[0m output \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(output, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m---> 19\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mfloat), label\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mfloat),  requires_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     20\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\guilh\\code\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: CrossEntropyLoss.forward() got an unexpected keyword argument 'requires_grad'"
     ]
    }
   ],
   "source": [
    "def train(\n",
    "    network,\n",
    "    train_loader,\n",
    "    criterion = nn.CrossEntropyLoss(),\n",
    "    optimizer = torch.optim.SGD,\n",
    "    num_epochs = 10,\n",
    "    learning_rate = 2.5e-4,\n",
    "):\n",
    "    optimizer = optimizer(network.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch_idx, sample in enumerate(train_loader):\n",
    "            data, label = sample\n",
    "            optimizer.zero_grad()\n",
    "            output = network(data)\n",
    "            output = F.softmax(output, dim=-1).argmax(dim=-1)[:,-1]\n",
    "            loss = criterion(output.to(torch.float), label.to(torch.float))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss\n",
    "\n",
    "        print(f\"epoch: {epoch}, loss: {epoch_loss}\")\n",
    "\n",
    "train(gptx, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b61a012e6ee0086db15c2aab5a8ce92bc403e7ba3f5fce645a2738ec1c41328d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
