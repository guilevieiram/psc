{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "I'll try to implement the masked attention transformer component to better understand the architecture involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating test embedding system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7680, 0.2334, 0.1617, 0.1945, 0.5311, 0.8125, 0.3190, 0.9987, 0.8926,\n",
       "         0.1501],\n",
       "        [0.5814, 0.1674, 0.2023, 0.6712, 0.2358, 0.2102, 0.3119, 0.7267, 0.9835,\n",
       "         0.6880],\n",
       "        [0.7755, 0.5037, 0.4213, 0.9135, 0.0250, 0.0136, 0.8709, 0.1924, 0.6385,\n",
       "         0.6570],\n",
       "        [0.9843, 0.9465, 0.9856, 0.9258, 0.8279, 0.8540, 0.0644, 0.3804, 0.0136,\n",
       "         0.3435],\n",
       "        [0.1457, 0.3402, 0.9064, 0.7949, 0.8369, 0.7340, 0.2298, 0.5078, 0.1739,\n",
       "         0.1443],\n",
       "        [0.9441, 0.5629, 0.0017, 0.7936, 0.3271, 0.8456, 0.4251, 0.1972, 0.5576,\n",
       "         0.3225]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we create a test embedding\n",
    "EMBEDDING_SIZE = 10 \n",
    "CONTEXT_SIZE = 3\n",
    "words = [ \n",
    "    \"I\",\n",
    "    \"had\",\n",
    "    \"a\",\n",
    "    \"dream\",\n",
    "    \"machines\",\n",
    "    \"learning\"\n",
    "]\n",
    "\n",
    "wte = torch.rand([len(words), EMBEDDING_SIZE])\n",
    "wte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4486, 0.2670, 0.4196, 0.3424, 0.4579, 0.7168, 0.4524, 0.1003, 0.6377,\n",
       "         0.9505],\n",
       "        [0.1465, 0.0730, 0.5992, 0.8240, 0.4818, 0.5363, 0.9028, 0.8181, 0.0449,\n",
       "         0.9999],\n",
       "        [0.0170, 0.0594, 0.6006, 0.0494, 0.6601, 0.9095, 0.4197, 0.6633, 0.6130,\n",
       "         0.3377]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy positional encoding matrix\n",
    "wpe = torch.rand([CONTEXT_SIZE, EMBEDDING_SIZE])\n",
    "wpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position (word: str) -> int: \n",
    "    try: \n",
    "        return words.index(word)\n",
    "    except ValueError as e: \n",
    "        raise Exception(\"Word not in vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0300, 0.4343, 0.6219, 1.0136, 0.6937, 0.9271, 0.7644, 0.8270, 1.6212,\n",
       "         1.6385],\n",
       "        [0.2922, 0.4132, 1.5056, 1.6188, 1.3187, 1.2703, 1.1325, 1.3259, 0.2188,\n",
       "         1.1442]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context embedding\n",
    "def embed(words_list: \"list[str]\") -> torch.tensor: \n",
    "    assert len(words_list) <= CONTEXT_SIZE, f\"Vector should have at max size {CONTEXT_SIZE}\"\n",
    "    positions = [get_position(word) for word in words_list]\n",
    "    tokens = wte[positions]\n",
    "    return tokens + wpe[0: len(words_list)]\n",
    "\n",
    "embed([\"had\", \"machines\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "had\n"
     ]
    }
   ],
   "source": [
    "def get_prob_distribution (word: str): \n",
    "    emb = embed([word])\n",
    "    logits = torch.matmul(emb, wte.transpose(0, -1))\n",
    "    prob = logits.softmax(1).flatten()\n",
    "    return prob \n",
    "\n",
    "probs  = get_prob_distribution(\"had\")\n",
    "print(words[probs.argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wq = torch.rand([EMBEDDING_SIZE, EMBEDDING_SIZE])\n",
    "Wk = torch.rand([EMBEDDING_SIZE, EMBEDDING_SIZE])\n",
    "Wv = torch.rand([EMBEDDING_SIZE, EMBEDDING_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKV:\n",
    "    def __init__ (self, token):\n",
    "        self.q = Wq @ token.flatten()\n",
    "        self.k = Wk @ token.flatten()\n",
    "        self.v = Wv @ token.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([4.0388, 4.5731, 4.0860, 6.4205, 5.6079, 3.5631, 5.5232, 6.3536, 5.7298,\n",
       "         5.1143]),\n",
       " tensor([4.0390, 4.5730, 4.0860, 6.4208, 5.6080, 3.5630, 5.5233, 6.3535, 5.7299,\n",
       "         5.1142])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_attention(toks: \"list[torch.tensor]\") -> \"list[torch.Tensor]\":\n",
    "    qkv = [QKV(tok) for tok in toks]\n",
    "    res = []\n",
    "    for x in qkv: \n",
    "        query = x.q\n",
    "        keys = [_.k for _ in qkv]\n",
    "        values = [_.v for _ in qkv]\n",
    "        scores = [query @ key for key in keys]\n",
    "        scores = torch.tensor(scores).softmax(0)\n",
    "        values = [value * score for value, score in zip(values, scores)]\n",
    "        res.append(sum(values))\n",
    "    return res\n",
    "\n",
    "process_attention( embed([\"I\", \"had\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.0388, 4.5731, 4.0860, 6.4205, 5.6079, 3.5631, 5.5232, 6.3536, 5.7298,\n",
       "         5.1143],\n",
       "        [4.0390, 4.5730, 4.0860, 6.4208, 5.6080, 3.5630, 5.5233, 6.3535, 5.7299,\n",
       "         5.1142]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_attention_matrix(toks: torch.tensor) -> torch.tensor: \n",
    "    Q = toks @ Wq.transpose(0, -1)\n",
    "    K = toks @ Wk.transpose(0, -1)\n",
    "    V = toks @ Wv.transpose(0, -1)\n",
    "\n",
    "    scores = Q @ K.transpose(0, -1)\n",
    "    scores = scores.softmax(1)\n",
    "\n",
    "    return scores @ V\n",
    "\n",
    "process_attention_matrix(embed([\"I\", \"had\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.3705, 4.9609, 4.1281, 4.7428, 4.8102, 3.9217, 4.7669, 6.7278, 5.0142,\n",
       "         5.5472],\n",
       "        [4.0390, 4.5730, 4.0860, 6.4208, 5.6080, 3.5630, 5.5233, 6.3535, 5.7299,\n",
       "         5.1142],\n",
       "        [4.0368, 4.5743, 4.0862, 6.4155, 5.6055, 3.5642, 5.5209, 6.3547, 5.7276,\n",
       "         5.1156]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def masked_attention(tokens: torch.tensor) -> torch.tensor: \n",
    "    Q = tokens @ Wq.transpose(0, -1)\n",
    "    K = tokens @ Wk.transpose(0, -1)\n",
    "    V = tokens @ Wv.transpose(0, -1)\n",
    "\n",
    "    scores = Q @ K.transpose(0, -1)\n",
    "    mask = torch.triu(\n",
    "        torch.ones_like(scores) * float(\"-inf\"),\n",
    "        diagonal=1\n",
    "    )\n",
    "    masked_scores = scores + mask\n",
    "\n",
    "    normal_masked_scores = masked_scores.softmax(1)\n",
    "\n",
    "    return normal_masked_scores @ V\n",
    "\n",
    "masked_attention(embed(['I','had', 'machines']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating a feed forward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = torch.rand([EMBEDDING_SIZE * 4, EMBEDDING_SIZE])\n",
    "layer2 = torch.rand([EMBEDDING_SIZE * 4, EMBEDDING_SIZE * 4])\n",
    "layer3 = torch.rand([EMBEDDING_SIZE , EMBEDDING_SIZE * 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10260.8418, 10693.3145,  9262.3232,  9490.9883, 10262.4023,  7892.1362,\n",
       "         10434.8086, 10779.0645,  9501.3799,  9467.4678],\n",
       "        [12799.6396, 13344.5430, 11554.7031, 11838.2559, 12806.6699,  9844.9766,\n",
       "         13015.1592, 13447.1670, 11855.3457, 11810.3477],\n",
       "        [12799.6396, 13344.5430, 11554.7031, 11838.2559, 12806.6699,  9844.9766,\n",
       "         13015.1592, 13447.1670, 11855.3457, 11810.3477]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ff(input: torch.tensor) -> torch.tensor:\n",
    "    output = layer3 @ layer2 @ layer1 @ input.transpose(0, -1)\n",
    "    return output.transpose(0, -1)\n",
    "\n",
    "ff(\n",
    "    masked_attention(\n",
    "        embed([\"I\", \"dream\", \"learning\"])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything in a decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3297, 0.3437, 0.2977, 0.3050, 0.3298, 0.2537, 0.3354, 0.3465, 0.3054,\n",
       "         0.3042],\n",
       "        [0.3298, 0.3438, 0.2977, 0.3050, 0.3299, 0.2536, 0.3353, 0.3464, 0.3054,\n",
       "         0.3043]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(tokens): \n",
    "    masked = masked_attention(tokens)\n",
    "    forward = ff(masked)\n",
    "    return normalize(forward)\n",
    "\n",
    "decode(torch.rand([2, 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model dummy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model(words_list, decoder_heads = 12): \n",
    "    tokens = embed(words_list)\n",
    "    for _ in range(decoder_heads): tokens = decode(tokens)\n",
    "\n",
    "    logits =  tokens @ wte.transpose(0, -1)\n",
    "    probs = logits.softmax(1)\n",
    "\n",
    "    return probs.argmax(1)\n",
    "\n",
    "model(\n",
    "    [\"I\", \"dream\", \"learning\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6841be0e1425757ac9eec34b2c6f490e3642beb3540581c52b3a77b850c12e89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
